"""
T2Iè¯„ä¼°è„šæœ¬
è®¡ç®—FIDã€ISã€CLIP Scoreç­‰ç»Ÿè®¡é‡
"""
import os
import argparse
import numpy as np
import torch
import json
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import pandas as pd


def calculate_fid(generated_dir: str, real_dir: str, batch_size: int = 50, device: str = "cuda"):
    """
    è®¡ç®—FID (FrÃ©chet Inception Distance)
    
    Args:
        generated_dir: ç”Ÿæˆå›¾ç‰‡ç›®å½•
        real_dir: çœŸå®å›¾ç‰‡ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡
    
    Returns:
        fid_score: FIDåˆ†æ•°
    """
    try:
        from pytorch_fid import fid_score
        
        print(f"ğŸ“Š è®¡ç®—FID...")
        print(f"   ç”Ÿæˆå›¾ç‰‡: {generated_dir}")
        print(f"   çœŸå®å›¾ç‰‡: {real_dir}")
        
        fid_value = fid_score.calculate_fid_given_paths(
            [generated_dir, real_dir],
            batch_size=batch_size,
            device=device,
            dims=2048
        )
        
        print(f"âœ… FID: {fid_value:.4f}")
        return fid_value
    
    except ImportError:
        print(f"âŒ æœªå®‰è£…pytorch-fidï¼Œè·³è¿‡FIDè®¡ç®—")
        print(f"   å®‰è£…å‘½ä»¤: pip install pytorch-fid")
        return None
    except Exception as e:
        print(f"âŒ FIDè®¡ç®—å¤±è´¥: {e}")
        return None


def calculate_inception_score(generated_dir: str, batch_size: int = 32, splits: int = 10, device: str = "cuda"):
    """
    è®¡ç®—Inception Score
    
    Args:
        generated_dir: ç”Ÿæˆå›¾ç‰‡ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        splits: åˆ†å‰²æ•°
        device: è®¾å¤‡
    
    Returns:
        is_mean: ISå‡å€¼
        is_std: ISæ ‡å‡†å·®
    """
    try:
        from torchmetrics.image.inception import InceptionScore
        from torchvision import transforms
        
        print(f"ğŸ“Š è®¡ç®—Inception Score...")
        
        # åˆå§‹åŒ–ISè®¡ç®—å™¨
        inception = InceptionScore(normalize=True).to(device)
        
        # åŠ è½½å›¾ç‰‡
        image_files = sorted([f for f in os.listdir(generated_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])
        
        transform = transforms.Compose([
            transforms.Resize((299, 299)),
            transforms.ToTensor(),
        ])
        
        all_images = []
        for img_file in tqdm(image_files, desc="åŠ è½½å›¾ç‰‡"):
            img_path = os.path.join(generated_dir, img_file)
            img = Image.open(img_path).convert('RGB')
            img_tensor = transform(img)
            all_images.append(img_tensor)
        
        # æ‰¹å¤„ç†è®¡ç®—
        for i in tqdm(range(0, len(all_images), batch_size), desc="è®¡ç®—IS"):
            batch = torch.stack(all_images[i:i+batch_size]).to(device)
            inception.update(batch)
        
        is_mean, is_std = inception.compute()
        
        print(f"âœ… Inception Score: {is_mean:.4f} Â± {is_std:.4f}")
        return is_mean.item(), is_std.item()
    
    except ImportError:
        print(f"âŒ æœªå®‰è£…torchmetricsï¼Œè·³è¿‡ISè®¡ç®—")
        print(f"   å®‰è£…å‘½ä»¤: pip install torchmetrics")
        return None, None
    except Exception as e:
        print(f"âŒ ISè®¡ç®—å¤±è´¥: {e}")
        return None, None


def calculate_clip_score(generated_dir: str, prompts_csv: str, batch_size: int = 32, device: str = "cuda"):
    """
    è®¡ç®—CLIP Score
    
    Args:
        generated_dir: ç”Ÿæˆå›¾ç‰‡ç›®å½•
        prompts_csv: prompts CSVæ–‡ä»¶
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡
    
    Returns:
        clip_score_mean: CLIP Scoreå‡å€¼
        clip_score_std: CLIP Scoreæ ‡å‡†å·®
    """
    try:
        import clip
        from PIL import Image
        
        print(f"ğŸ“Š è®¡ç®—CLIP Score...")
        
        # åŠ è½½CLIPæ¨¡å‹
        model, preprocess = clip.load("ViT-B/32", device=device)
        
        # è¯»å–prompts
        df = pd.read_csv(prompts_csv)
        prompt_column = 'prompt' if 'prompt' in df.columns else 'caption'
        
        # è·å–å›¾ç‰‡æ–‡ä»¶
        image_files = sorted([f for f in os.listdir(generated_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])
        
        # åŒ¹é…å›¾ç‰‡å’Œprompt
        scores = []
        
        for img_file in tqdm(image_files, desc="è®¡ç®—CLIP Score"):
            # ä»æ–‡ä»¶åæå–image_id
            # å‡è®¾æ–‡ä»¶åæ ¼å¼: {image_id}_seed{seed}.png
            image_id = img_file.split('_')[0]
            
            # æŸ¥æ‰¾å¯¹åº”çš„prompt
            try:
                image_id_int = int(image_id)
                prompt_row = df[df['image_id'] == image_id_int]
                if len(prompt_row) == 0:
                    continue
                prompt = prompt_row.iloc[0][prompt_column]
            except:
                continue
            
            # åŠ è½½å›¾ç‰‡
            img_path = os.path.join(generated_dir, img_file)
            image = preprocess(Image.open(img_path).convert('RGB')).unsqueeze(0).to(device)
            
            # ç¼–ç æ–‡æœ¬
            text = clip.tokenize([prompt]).to(device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            with torch.no_grad():
                image_features = model.encode_image(image)
                text_features = model.encode_text(text)
                
                # å½’ä¸€åŒ–
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = (image_features @ text_features.T).item()
                scores.append(similarity)
        
        clip_score_mean = np.mean(scores)
        clip_score_std = np.std(scores)
        
        print(f"âœ… CLIP Score: {clip_score_mean:.4f} Â± {clip_score_std:.4f}")
        return clip_score_mean, clip_score_std
    
    except ImportError:
        print(f"âŒ æœªå®‰è£…CLIPï¼Œè·³è¿‡CLIP Scoreè®¡ç®—")
        print(f"   å®‰è£…å‘½ä»¤: pip install git+https://github.com/openai/CLIP.git")
        return None, None
    except Exception as e:
        print(f"âŒ CLIP Scoreè®¡ç®—å¤±è´¥: {e}")
        return None, None


def calculate_lpips(generated_dir: str, real_dir: str, num_samples: int = 1000, device: str = "cuda"):
    """
    è®¡ç®—LPIPS (Learned Perceptual Image Patch Similarity)
    
    Args:
        generated_dir: ç”Ÿæˆå›¾ç‰‡ç›®å½•
        real_dir: çœŸå®å›¾ç‰‡ç›®å½•
        num_samples: é‡‡æ ·æ•°é‡
        device: è®¾å¤‡
    
    Returns:
        lpips_mean: LPIPSå‡å€¼
        lpips_std: LPIPSæ ‡å‡†å·®
    """
    try:
        import lpips
        from torchvision import transforms
        
        print(f"ğŸ“Š è®¡ç®—LPIPS...")
        
        # åˆå§‹åŒ–LPIPSæ¨¡å‹
        loss_fn = lpips.LPIPS(net='alex').to(device)
        
        # åŠ è½½å›¾ç‰‡
        gen_files = sorted([f for f in os.listdir(generated_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])
        real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])
        
        # é‡‡æ ·
        import random
        random.seed(42)
        gen_files = random.sample(gen_files, min(num_samples, len(gen_files)))
        real_files = random.sample(real_files, min(num_samples, len(real_files)))
        
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        scores = []
        for gen_file, real_file in tqdm(zip(gen_files, real_files), total=len(gen_files), desc="è®¡ç®—LPIPS"):
            gen_img = Image.open(os.path.join(generated_dir, gen_file)).convert('RGB')
            real_img = Image.open(os.path.join(real_dir, real_file)).convert('RGB')
            
            gen_tensor = transform(gen_img).unsqueeze(0).to(device)
            real_tensor = transform(real_img).unsqueeze(0).to(device)
            
            with torch.no_grad():
                dist = loss_fn(gen_tensor, real_tensor)
                scores.append(dist.item())
        
        lpips_mean = np.mean(scores)
        lpips_std = np.std(scores)
        
        print(f"âœ… LPIPS: {lpips_mean:.4f} Â± {lpips_std:.4f}")
        return lpips_mean, lpips_std
    
    except ImportError:
        print(f"âŒ æœªå®‰è£…lpipsï¼Œè·³è¿‡LPIPSè®¡ç®—")
        print(f"   å®‰è£…å‘½ä»¤: pip install lpips")
        return None, None
    except Exception as e:
        print(f"âŒ LPIPSè®¡ç®—å¤±è´¥: {e}")
        return None, None


def calculate_basic_stats(generated_dir: str):
    """
    è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        generated_dir: ç”Ÿæˆå›¾ç‰‡ç›®å½•
    
    Returns:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    print(f"ğŸ“Š è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯...")
    
    image_files = [f for f in os.listdir(generated_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
    
    # ç»Ÿè®¡å›¾ç‰‡å°ºå¯¸
    sizes = []
    for img_file in tqdm(image_files[:100], desc="é‡‡æ ·å›¾ç‰‡å°ºå¯¸"):  # åªé‡‡æ ·100å¼ 
        img_path = os.path.join(generated_dir, img_file)
        img = Image.open(img_path)
        sizes.append(img.size)
    
    stats = {
        'num_images': len(image_files),
        'image_sizes': sizes[:10],  # åªè®°å½•å‰10ä¸ª
        'common_size': max(set(sizes), key=sizes.count) if sizes else None
    }
    
    print(f"âœ… åŸºæœ¬ç»Ÿè®¡:")
    print(f"   å›¾ç‰‡æ•°é‡: {stats['num_images']}")
    print(f"   å¸¸è§å°ºå¯¸: {stats['common_size']}")
    
    return stats


def main():
    parser = argparse.ArgumentParser(description="T2Iè¯„ä¼°è„šæœ¬")
    
    # è¾“å…¥è·¯å¾„
    parser.add_argument("--generated_dir", type=str, required=True,
                        help="ç”Ÿæˆå›¾ç‰‡ç›®å½•")
    parser.add_argument("--real_dir", type=str, default=None,
                        help="çœŸå®å›¾ç‰‡ç›®å½•ï¼ˆç”¨äºFIDå’ŒLPIPSï¼‰")
    parser.add_argument("--prompts_csv", type=str, default=None,
                        help="Prompts CSVæ–‡ä»¶ï¼ˆç”¨äºCLIP Scoreï¼‰")
    
    # è¯„ä¼°é€‰é¡¹
    parser.add_argument("--metrics", type=str, nargs='+', 
                        default=['fid', 'is', 'clip', 'lpips'],
                        choices=['fid', 'is', 'clip', 'lpips', 'all'],
                        help="è¦è®¡ç®—çš„æŒ‡æ ‡")
    parser.add_argument("--batch_size", type=int, default=32,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--device", type=str, default="cuda",
                        help="è®¾å¤‡")
    
    # è¾“å‡º
    parser.add_argument("--output_json", type=str, default=None,
                        help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print(f"{'='*60}")
    print(f"ğŸ“Š T2Iè¯„ä¼°å·¥å…·")
    print(f"{'='*60}\n")
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(args.generated_dir):
        print(f"âŒ ç”Ÿæˆå›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {args.generated_dir}")
        return
    
    # å¤„ç†metrics
    if 'all' in args.metrics:
        args.metrics = ['fid', 'is', 'clip', 'lpips']
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
    basic_stats = calculate_basic_stats(args.generated_dir)
    
    # åˆå§‹åŒ–ç»“æœ
    results = {
        'generated_dir': args.generated_dir,
        'num_images': basic_stats['num_images'],
        'metrics': {}
    }
    
    # è®¡ç®—FID
    if 'fid' in args.metrics:
        if args.real_dir is None:
            print(f"âš ï¸  è·³è¿‡FID: æœªæä¾›çœŸå®å›¾ç‰‡ç›®å½•")
        else:
            print(f"\n{'='*60}")
            fid_value = calculate_fid(args.generated_dir, args.real_dir, args.batch_size, args.device)
            if fid_value is not None:
                results['metrics']['fid'] = fid_value
    
    # è®¡ç®—Inception Score
    if 'is' in args.metrics:
        print(f"\n{'='*60}")
        is_mean, is_std = calculate_inception_score(args.generated_dir, args.batch_size, device=args.device)
        if is_mean is not None:
            results['metrics']['inception_score'] = {
                'mean': is_mean,
                'std': is_std
            }
    
    # è®¡ç®—CLIP Score
    if 'clip' in args.metrics:
        if args.prompts_csv is None:
            print(f"âš ï¸  è·³è¿‡CLIP Score: æœªæä¾›prompts CSV")
        else:
            print(f"\n{'='*60}")
            clip_mean, clip_std = calculate_clip_score(args.generated_dir, args.prompts_csv, args.batch_size, args.device)
            if clip_mean is not None:
                results['metrics']['clip_score'] = {
                    'mean': clip_mean,
                    'std': clip_std
                }
    
    # è®¡ç®—LPIPS
    if 'lpips' in args.metrics:
        if args.real_dir is None:
            print(f"âš ï¸  è·³è¿‡LPIPS: æœªæä¾›çœŸå®å›¾ç‰‡ç›®å½•")
        else:
            print(f"\n{'='*60}")
            lpips_mean, lpips_std = calculate_lpips(args.generated_dir, args.real_dir, device=args.device)
            if lpips_mean is not None:
                results['metrics']['lpips'] = {
                    'mean': lpips_mean,
                    'std': lpips_std
                }
    
    # è¾“å‡ºç»“æœ
    print(f"\n{'='*60}")
    print(f"âœ… è¯„ä¼°å®Œæˆ!")
    print(f"{'='*60}")
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(json.dumps(results, indent=2, ensure_ascii=False))
    
    # ä¿å­˜ç»“æœ
    if args.output_json:
        os.makedirs(os.path.dirname(args.output_json) or '.', exist_ok=True)
        with open(args.output_json, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output_json}")
    else:
        # é»˜è®¤ä¿å­˜åˆ°ç”Ÿæˆç›®å½•
        default_output = os.path.join(args.generated_dir, "evaluation_results.json")
        with open(default_output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {default_output}")


if __name__ == "__main__":
    main()

