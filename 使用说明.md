# WAN2.2-5B 模型使用说明

## 概述
这是WAN2.2-5B（Text-to-Image-to-Video）模型的代码仓库。此仓库**仅包含模型代码和配置文件**，不包含模型权重，以避免本地存储和运行大型模型。

## 目录结构
```
Wan2.2-TI2V-5B/
├── README.md                          # 官方说明文档
├── config.json                        # 模型配置文件
├── configuration.json                 # 配置信息
├── diffusion_pytorch_model.safetensors.index.json  # 权重索引文件
├── download_model_weights.py          # Python下载脚本
├── download_model_weights.bat         # Windows批处理下载脚本
├── 使用说明.md                        # 本文档
├── assets/                            # 资源文件夹
├── examples/                          # 示例文件
└── google/umt5-xxl/                   # T5编码器配置
```

## 下载模型权重

### 方法一：使用Python脚本（推荐）
```bash
python download_model_weights.py
```

### 方法二：使用Windows批处理脚本
```bash
download_model_weights.bat
```

### 方法三：手动使用Hugging Face CLI
```bash
# 安装依赖
pip install huggingface_hub[cli]

# 下载到指定目录
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./model_weights
```

### 方法四：使用Git LFS（如果在仓库目录中）
```bash
# 安装Git LFS
git lfs install

# 拉取LFS文件
git lfs pull
```

## 重要注意事项

### ⚠️ 本地运行限制
- **严禁在本地运行此模型**
- 模型文件非常大（约10GB+）
- 需要大量GPU内存和计算资源
- 建议仅在远程服务器或云平台上运行

### 🌐 推荐的远程运行平台
1. **启智算力平台** - 提供模型训练和推理云服务
2. **共绩算力** - 支持容器化部署，提供GPU资源
3. **Google Colab Pro** - 适合实验和测试
4. **AWS/Azure/GCP** - 企业级云计算平台

### 📋 系统要求（仅供参考，请勿本地运行）
- GPU: NVIDIA RTX 4090 或更高（24GB+ VRAM）
- RAM: 32GB+
- 存储: 50GB+ 可用空间
- CUDA: 11.8+
- Python: 3.8+

## 模型信息
- **模型名称**: WAN2.2-TI2V-5B
- **模型类型**: Text-to-Image-to-Video 生成模型
- **参数数量**: 50亿
- **输入**: 文本描述
- **输出**: 图像序列/视频

## 许可证
请参考原始仓库的许可证文件。

## 🚀 推理加速技巧

### **CFG截断优化（新功能）**
WAN2.2现在支持CFG截断技术，可以在最后几步跳过条件前传来加速推理：

**⚠️ 重要更新**: CFG截断现在**默认禁用**（cfg_truncate_steps=0），需要手动启用。

```bash
# 启用CFG截断（手动开启）
python generate.py --task ti2v-5B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --prompt "Your prompt here"

# 27B MOE模型双重CFG截断
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt here"
```

### **时间记录功能（新增）**
现在推理会自动记录详细的时间信息：
- ⏱️ **模型加载耗时**: 包含所有模型组件的加载时间
- 🔄 **专家切换总耗时**: MOE模型所有专家切换的累计时间
- ⚡ **纯推理耗时**: 扣除专家切换后的真实推理时间
- 📊 **总推理耗时**: 包含专家切换的完整推理时间
- 📈 **推理速度**: 步/秒（采样步骤执行速度）
- 🎬 **每步耗时**: 秒/步（单步平均时间）

### **帧数支持**
- ✅ **支持任意帧数**: 包括1帧生成
- 🔧 **移除4n+1限制**: 现在可以自由设置帧数
- 💡 **推荐值**: 虽然限制已移除，但81帧仍是最优配置

### **输出文件组织**
每次推理会自动创建结构化的输出文件夹：
```
outputs/
├── t2v-A14B_20250919_143052_A_beautiful_sunset/
│   ├── t2v-A14B_1280x720_20250919_143052.mp4    # 生成的视频
│   └── inference_record.json                     # 推理记录
├── t2v-A14B_20250919_143158_Another_prompt/
│   ├── t2v-A14B_1280x720_20250919_143158.mp4
│   └── inference_record.json
└── ...
```

**记录文件包含**：
- 📝 完整的参数设置
- ⏱️ 详细的性能数据  
- 🔧 分布式配置信息
- 📊 推理效率统计

### **双重CFG截断参数说明**
**低噪声专家截断（最后几步）**：
- `--cfg_truncate_steps 3`: 激进加速，轻微质量损失
- `--cfg_truncate_steps 5`: 平衡模式（推荐）
- `--cfg_truncate_steps 8`: 保守加速，质量优先  
- `--cfg_truncate_steps 0`: 禁用低噪声截断

**高噪声专家截断（专家切换前）**：
- `--cfg_truncate_high_noise_steps 2`: 轻微加速
- `--cfg_truncate_high_noise_steps 3`: 平衡模式（推荐）
- `--cfg_truncate_high_noise_steps 5`: 激进加速
- `--cfg_truncate_high_noise_steps 0`: 禁用高噪声截断

### **组合使用示例**
```bash
# 双重CFG截断（最大加速）
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"

# 只截断高噪声专家
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 0 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"
```

### **快速加载优化**
```bash
# 启用快速加载模式（需要更多GPU内存）
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 1 \
    --prompt "Your prompt"
```

**快速加载功能**：
- 🚀 **并行加载**: 两个专家模型同时加载
- 🔥 **GPU预热**: 自动初始化CUDA环境
- 💾 **内存预分配**: 优化GPU内存管理
- 🚫 **禁用卸载**: 所有模型常驻GPU，避免切换开销

## 🌐 **多机分布式推理**

### **多机推理架构**
WAN2.2支持跨多台机器的分布式推理，通过NCCL后端实现高效的跨节点通信：

```bash
# 多机推理配置 (2台机器，每台4GPU)
# 机器1 (主节点):
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=0 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8

# 机器2 (工作节点):  
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=1 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8
```

### **环境变量说明**
- `MASTER_ADDR`: 主节点IP地址
- `MASTER_PORT`: 通信端口（通常29500）
- `WORLD_SIZE`: 总GPU数量（所有机器GPU总和）
- `NODE_RANK`: 当前机器编号（0为主节点）
- `--nnodes`: 机器总数
- `--nproc_per_node`: 每台机器的GPU数量

### **网络要求**
- **带宽**: 推荐10Gbps以上网络连接
- **延迟**: <1ms机器间延迟
- **端口**: 确保29500端口开放
- **防火墙**: 关闭或配置允许NCCL通信

### **性能提升**
- **单独低噪声截断**: 20-30%时间减少
- **单独高噪声截断**: 15-25%时间减少  
- **双重截断**: 35-50%时间减少
- **快速加载**: 50-70%加载时间减少
- **多机分布式**: 接近线性扩展（2-4倍加速）
- **质量影响**: 最小，主要在细节层面
- **适用场景**: 所有WAN2.2 MOE模型（27B）

### **多机配置示例**

**2机8卡配置**:
```bash
# 总计算资源: 2台机器 × 4GPU = 8GPU
# 参数分片: 27B ÷ 8 = 3.375GB/GPU
# 序列并行: ulysses_size=8
# 预期加速: 6-7倍（相比单GPU）
```

**4机16卡配置**:
```bash
# 总计算资源: 4台机器 × 4GPU = 16GPU  
# 参数分片: 27B ÷ 16 = 1.6875GB/GPU
# 序列并行: ulysses_size=16
# 预期加速: 12-14倍（相比单GPU）
```

## 🔧 **常见问题和解决方案**

### **多GPU分布式错误处理**

**SIGSEGV (exitcode: -11) 和 SIGABRT (exitcode: -6) 错误**：
这些错误通常在多GPU FSDP环境中出现，主要由并发加载冲突引起。

**解决方案**：

```bash
# 方案1: 保守内存 + 线程优化
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
torchrun --nproc_per_node=4 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 4 \
    --frame_num 121

# 方案2: 降低GPU数量
torchrun --nproc_per_node=2 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 2 \
    --frame_num 121

# 方案3: 单GPU稳定模式
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 121

# 方案4: 启用模型卸载（最保守）
python generate.py --task t2v-A14B \
    --offload_model True --convert_model_dtype --t5_cpu \
    --frame_num 121
```

**错误发生时机**: 通常在FSDP模型分片初始化阶段，而不是推理阶段。

### **CFG截断实际效果分析**
基于实际测试，CFG截断的加速效果约为10-15%，主要原因：

- **专家切换占主导**: 101秒切换 vs 133秒推理
- **CFG截断只优化推理部分**: 无法减少切换开销  
- **内存访问瓶颈**: 单次前传节省被其他操作抵消

**更有效的加速策略**：
1. **使用--fast_loading**: 消除101秒专家切换时间
2. **多GPU分片**: 3-4倍整体加速
3. **CFG截断**: 在前两者基础上额外10-15%加速

## 🧠 **前沿优化技术探索**

### **动态Token修剪 (Adaptive Token Pruning)**

**核心思想**：
在高噪声专家的后期阶段，动态停止变化小的token更新，让高噪声专家专注于结构性token，细节token留给低噪声专家处理。

**技术可行性分析**：

**✅ 架构优势**：
- WAN2.2使用标准Transformer结构，token独立可追踪
- 清晰的序列维度 `[B, L, C]`，便于token级别操作
- 已有专家切换机制，证明动态计算路径可行
- 支持序列并行，有token级别的分布式处理基础

**🔧 改进实现策略**：
```python
# 1. 专家限制：只在高噪声专家中应用
def should_apply_pruning(layer_idx, expert_name):
    return (expert_name == "high_noise" and 
            start_layer <= layer_idx <= end_layer)

# 2. 分数归一化：使三个维度量级相同
def normalize_scores(raw_scores):
    normalized = {}
    for score_type, value in raw_scores.items():
        stats = score_stats[score_type]
        # Min-Max归一化到[0,1]
        normalized[score_type] = (value - stats['min']) / (stats['max'] - stats['min'])
    return normalized

# 3. 综合评分计算（归一化后）
def calculate_composite_score(token_idx, ...):
    # 原始分数
    raw_scores = {
        'change': torch.norm(current - prev, dim=-1),
        'self_attn': image_self_attn.mean(dim=0).sum(),
        'cross_attn': cross_attn.mean(dim=0).sum()
    }
    
    # 归一化分数（量级统一）
    norm_scores = normalize_scores(raw_scores)
    
    # 加权综合评分（0-1范围）
    composite_score = (
        0.4 * norm_scores['change'] +      # 40% 变化幅度
        0.3 * norm_scores['self_attn'] +   # 30% 图像自注意力  
        0.3 * norm_scores['cross_attn']    # 30% 跨模态注意力
    )
    
    return composite_score

# 4. 渐进式修剪：固定阈值，逐步增加修剪
def progressive_pruning(layer_idx, expert_name):
    if expert_name != "high_noise":
        return all_active_mask  # 低噪声专家不修剪
        
    if layer_idx < start_layer:
        return all_active_mask  # 前期完全推理
        
    # 计算所有token评分
    for token_idx in image_tokens:
        score = calculate_composite_score(token_idx, ...)
        
        # 固定阈值修剪（如0.5）
        if score < pruning_threshold:
            frozen_tokens.add(token_idx)  # 累积冻结
            
    return create_active_mask(frozen_tokens)

# 5. 渐进式效果
# Layer 15: 100% token激活 (开始修剪)
# Layer 20: 80% token激活  (低分token被修剪)
# Layer 25: 60% token激活  (更多token被修剪)
# Layer 30: 40% token激活  (大部分token被修剪)
# Layer 35: 30% token激活  (修剪结束)
# 低噪声专家: 处理所有token (包括被修剪的)
```

**💡 改进后的预期收益**：

**🎯 专家限制优势**：
- **精准定位**: 只在高噪声专家中修剪，避免影响低噪声专家的细节处理
- **专家分工**: 高噪声专家专注重要token，低噪声专家处理完整信息

**📊 归一化优势**：
- **量级统一**: 三个评分维度归一化到[0,1]，避免某一维度主导
- **权重平衡**: 40%变化+30%自注意力+30%跨模态的平衡更加准确
- **阈值稳定**: 固定阈值0.5更直观，便于调优

**⚡ 渐进式修剪优势**：
- **平滑过渡**: 从完全推理逐步减少到部分推理，避免突然的质量下降
- **累积效应**: 一旦被修剪的token保持冻结，计算量持续减少
- **可控性强**: 通过调整start_layer和pruning_threshold精确控制修剪程度

**📈 渐进式修剪时间线**：
```
推理步骤 1-14:  100% token激活 (完全推理建立基础)
推理步骤 15-19: 90%+ token激活 (开始修剪低分token)
推理步骤 20-24: 70-80% token激活 (加速修剪)
推理步骤 25-29: 50-60% token激活 (大幅修剪)
推理步骤 30-35: 30-40% token激活 (深度修剪)
专家切换后:    100% token激活 (低噪声专家处理全部)
```

**🔬 三维评分 + 归一化优势**：
- **变化幅度 (归一化)**: 识别仍在演化的动态区域，不受绝对值影响
- **自注意力 (归一化)**: 识别图像内部结构关键点，消除尺度差异  
- **跨模态注意力 (归一化)**: 识别与文本强相关区域，统一量级比较

**🎯 实现难点**：
- 需要修改attention机制以支持部分token更新
- KV缓存管理复杂度增加
- 与FSDP/序列并行的兼容性
- 动态mask的梯度处理

**📊 实现优先级**: 中等到高
- 技术可行性强，但需要深度修改attention层
- 建议先在单GPU环境下原型验证
- 成功后再适配多GPU分布式环境

## 技术支持
如有问题，请参考：
1. 官方README.md文档
2. Hugging Face模型页面: https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B
3. 相关技术论文和文档

---
**再次提醒：此仓库仅用于代码研究，请勿在本地运行模型！**

