# WAN2.2-5B 模型使用说明

## 概述
这是WAN2.2-5B（Text-to-Image-to-Video）模型的代码仓库。此仓库**仅包含模型代码和配置文件**，不包含模型权重，以避免本地存储和运行大型模型。

## 目录结构
```
Wan2.2-TI2V-5B/
├── README.md                          # 官方说明文档
├── config.json                        # 模型配置文件
├── configuration.json                 # 配置信息
├── diffusion_pytorch_model.safetensors.index.json  # 权重索引文件
├── download_model_weights.py          # Python下载脚本
├── download_model_weights.bat         # Windows批处理下载脚本
├── 使用说明.md                        # 本文档
├── assets/                            # 资源文件夹
├── examples/                          # 示例文件
└── google/umt5-xxl/                   # T5编码器配置
```

## 下载模型权重

### 方法一：使用Python脚本（推荐）
```bash
python download_model_weights.py
```

### 方法二：使用Windows批处理脚本
```bash
download_model_weights.bat
```

### 方法三：手动使用Hugging Face CLI
```bash
# 安装依赖
pip install huggingface_hub[cli]

# 下载到指定目录
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./model_weights
```

### 方法四：使用Git LFS（如果在仓库目录中）
```bash
# 安装Git LFS
git lfs install

# 拉取LFS文件
git lfs pull
```

## 重要注意事项

### ⚠️ 本地运行限制
- **严禁在本地运行此模型**
- 模型文件非常大（约10GB+）
- 需要大量GPU内存和计算资源
- 建议仅在远程服务器或云平台上运行

### 🌐 推荐的远程运行平台
1. **启智算力平台** - 提供模型训练和推理云服务
2. **共绩算力** - 支持容器化部署，提供GPU资源
3. **Google Colab Pro** - 适合实验和测试
4. **AWS/Azure/GCP** - 企业级云计算平台

### 📋 系统要求（仅供参考，请勿本地运行）
- GPU: NVIDIA RTX 4090 或更高（24GB+ VRAM）
- RAM: 32GB+
- 存储: 50GB+ 可用空间
- CUDA: 11.8+
- Python: 3.8+

## 模型信息
- **模型名称**: WAN2.2-TI2V-5B
- **模型类型**: Text-to-Image-to-Video 生成模型
- **参数数量**: 50亿
- **输入**: 文本描述
- **输出**: 图像序列/视频

## 许可证
请参考原始仓库的许可证文件。

## 🚀 推理加速技巧

### **CFG截断优化（新功能）**
WAN2.2现在支持CFG截断技术，可以在最后几步跳过条件前传来加速推理：

**⚠️ 重要更新**: CFG截断现在**默认禁用**（cfg_truncate_steps=0），需要手动启用。

```bash
# 启用CFG截断（手动开启）
python generate.py --task ti2v-5B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --prompt "Your prompt here"

# 27B MOE模型双重CFG截断
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt here"
```

### **时间记录功能（新增）**
现在推理会自动记录详细的时间信息：
- ⏱️ **模型加载耗时**: 包含所有模型组件的加载时间
- 🔄 **专家切换总耗时**: MOE模型所有专家切换的累计时间
- ⚡ **纯推理耗时**: 扣除专家切换后的真实推理时间
- 📊 **总推理耗时**: 包含专家切换的完整推理时间
- 📈 **推理速度**: 步/秒（采样步骤执行速度）
- 🎬 **每步耗时**: 秒/步（单步平均时间）

### **帧数支持**
- ✅ **支持任意帧数**: 包括1帧生成
- 🔧 **移除4n+1限制**: 现在可以自由设置帧数
- 💡 **推荐值**: 虽然限制已移除，但81帧仍是最优配置

### **输出文件组织**
每次推理会自动创建结构化的输出文件夹：
```
outputs/
├── t2v-A14B_20250919_143052_A_beautiful_sunset/
│   ├── t2v-A14B_1280x720_20250919_143052.mp4    # 生成的视频
│   └── inference_record.json                     # 推理记录
├── t2v-A14B_20250919_143158_Another_prompt/
│   ├── t2v-A14B_1280x720_20250919_143158.mp4
│   └── inference_record.json
└── ...
```

**记录文件包含**：
- 📝 完整的参数设置
- ⏱️ 详细的性能数据  
- 🔧 分布式配置信息
- 📊 推理效率统计

### **双重CFG截断参数说明**
**低噪声专家截断（最后几步）**：
- `--cfg_truncate_steps 3`: 激进加速，轻微质量损失
- `--cfg_truncate_steps 5`: 平衡模式（推荐）
- `--cfg_truncate_steps 8`: 保守加速，质量优先  
- `--cfg_truncate_steps 0`: 禁用低噪声截断

**高噪声专家截断（专家切换前）**：
- `--cfg_truncate_high_noise_steps 2`: 轻微加速
- `--cfg_truncate_high_noise_steps 3`: 平衡模式（推荐）
- `--cfg_truncate_high_noise_steps 5`: 激进加速
- `--cfg_truncate_high_noise_steps 0`: 禁用高噪声截断

### **组合使用示例**
```bash
# 双重CFG截断（最大加速）
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"

# 只截断高噪声专家
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 0 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"
```

### **快速加载优化**
```bash
# 启用快速加载模式（需要更多GPU内存）
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 1 \
    --prompt "Your prompt"
```

**快速加载功能**：
- 🚀 **并行加载**: 两个专家模型同时加载
- 🔥 **GPU预热**: 自动初始化CUDA环境
- 💾 **内存预分配**: 优化GPU内存管理
- 🚫 **禁用卸载**: 所有模型常驻GPU，避免切换开销

## 🌐 **多机分布式推理**

### **多机推理架构**
WAN2.2支持跨多台机器的分布式推理，通过NCCL后端实现高效的跨节点通信：

```bash
# 多机推理配置 (2台机器，每台4GPU)
# 机器1 (主节点):
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=0 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8

# 机器2 (工作节点):  
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=1 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8
```

### **环境变量说明**
- `MASTER_ADDR`: 主节点IP地址
- `MASTER_PORT`: 通信端口（通常29500）
- `WORLD_SIZE`: 总GPU数量（所有机器GPU总和）
- `NODE_RANK`: 当前机器编号（0为主节点）
- `--nnodes`: 机器总数
- `--nproc_per_node`: 每台机器的GPU数量

### **网络要求**
- **带宽**: 推荐10Gbps以上网络连接
- **延迟**: <1ms机器间延迟
- **端口**: 确保29500端口开放
- **防火墙**: 关闭或配置允许NCCL通信

### **性能提升**
- **单独低噪声截断**: 20-30%时间减少
- **单独高噪声截断**: 15-25%时间减少  
- **双重截断**: 35-50%时间减少
- **快速加载**: 50-70%加载时间减少
- **多机分布式**: 接近线性扩展（2-4倍加速）
- **质量影响**: 最小，主要在细节层面
- **适用场景**: 所有WAN2.2 MOE模型（27B）

### **多机配置示例**

**2机8卡配置**:
```bash
# 总计算资源: 2台机器 × 4GPU = 8GPU
# 参数分片: 27B ÷ 8 = 3.375GB/GPU
# 序列并行: ulysses_size=8
# 预期加速: 6-7倍（相比单GPU）
```

**4机16卡配置**:
```bash
# 总计算资源: 4台机器 × 4GPU = 16GPU  
# 参数分片: 27B ÷ 16 = 1.6875GB/GPU
# 序列并行: ulysses_size=16
# 预期加速: 12-14倍（相比单GPU）
```

## 🔧 **常见问题和解决方案**

### **多GPU分布式错误处理**

**SIGSEGV (exitcode: -11) 和 SIGABRT (exitcode: -6) 错误**：
这些错误通常在多GPU FSDP环境中出现，主要由并发加载冲突引起。

**解决方案**：

```bash
# 方案1: 保守内存 + 线程优化
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
torchrun --nproc_per_node=4 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 4 \
    --frame_num 121

# 方案2: 降低GPU数量
torchrun --nproc_per_node=2 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 2 \
    --frame_num 121

# 方案3: 单GPU稳定模式
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 121

# 方案4: 启用模型卸载（最保守）
python generate.py --task t2v-A14B \
    --offload_model True --convert_model_dtype --t5_cpu \
    --frame_num 121
```

**错误发生时机**: 通常在FSDP模型分片初始化阶段，而不是推理阶段。

### **CFG截断实际效果分析**
基于实际测试，CFG截断的加速效果约为10-15%，主要原因：

- **专家切换占主导**: 101秒切换 vs 133秒推理
- **CFG截断只优化推理部分**: 无法减少切换开销  
- **内存访问瓶颈**: 单次前传节省被其他操作抵消

**更有效的加速策略**：
1. **使用--fast_loading**: 消除101秒专家切换时间
2. **多GPU分片**: 3-4倍整体加速
3. **CFG截断**: 在前两者基础上额外10-15%加速

## 🧠 **前沿优化技术探索**

### **动态Token修剪 (Adaptive Token Pruning)**

**核心思想**：
在高噪声专家的后期阶段，动态停止变化小的token更新，让高噪声专家专注于结构性token，细节token留给低噪声专家处理。

**技术可行性分析**：

**✅ 架构优势**：
- WAN2.2使用标准Transformer结构，token独立可追踪
- 清晰的序列维度 `[B, L, C]`，便于token级别操作
- 已有专家切换机制，证明动态计算路径可行
- 支持序列并行，有token级别的分布式处理基础

**🔧 最终改进实现策略**：
```python
# 1. 专家限制 + 基准期：只在高噪声专家中应用，前5步完全推理
def should_apply_pruning(layer_idx, expert_name):
    return (expert_name == "high_noise" and 
            layer_idx > baseline_steps and     # 前5步完全推理
            layer_idx <= end_layer)

# 2. 量级对齐：将attention分数缩放到变化分数的量级
def scale_attention_scores_to_change_magnitude(self_attn, cross_attn):
    change_range = change_stats['max'] - change_stats['min']
    
    # 将attention分数线性缩放到change分数的量级
    scaled_self_attn = self_attn * (change_range / max(self_attn, 1e-8))
    scaled_cross_attn = cross_attn * (change_range / max(cross_attn, 1e-8))
    
    return scaled_self_attn, scaled_cross_attn

# 3. 综合评分计算（量级对齐后，保留分数趋势）
def calculate_composite_score(token_idx, ...):
    # 原始分数
    raw_scores = {
        'change': torch.norm(current - prev, dim=-1),           # 基准量级
        'self_attn': image_self_attn.mean(dim=0).sum(),
        'cross_attn': cross_attn.mean(dim=0).sum()
    }
    
    # 量级对齐（而非0-1归一化，保留整体趋势）
    scaled_self_attn, scaled_cross_attn = scale_attention_scores_to_change_magnitude(
        raw_scores['self_attn'], raw_scores['cross_attn'])
    
    # 加权综合评分（保留原始量级和趋势）
    composite_score = (
        0.4 * raw_scores['change'] +       # 40% 变化幅度（基准）
        0.3 * scaled_self_attn +           # 30% 缩放后自注意力  
        0.3 * scaled_cross_attn            # 30% 缩放后跨模态注意力
    )
    
    return composite_score

# 4. 动态阈值确定：第5步最低x%分数作为阈值
def determine_dynamic_threshold(layer_5_scores, percentile_x=20):
    # 在第5步收集所有token的综合评分
    if layer_idx == 5:
        baseline_scores = [score for token_idx, score in layer_5_scores]
        # 计算第x百分位数作为修剪阈值
        dynamic_threshold = np.percentile(baseline_scores, percentile_x)
        print(f"动态阈值: {dynamic_threshold:.4f} (第{percentile_x}百分位数)")
        
    return dynamic_threshold

# 5. 渐进式修剪：基于第5步确定的动态阈值
def progressive_pruning(layer_idx, expert_name):
    if layer_idx <= 5:
        return all_active_mask  # 前5步完全推理，建立基准
        
    if expert_name != "high_noise":
        return all_active_mask  # 低噪声专家不修剪
        
    # 使用第5步确定的动态阈值进行修剪
    for token_idx in image_tokens:
        score = calculate_composite_score(token_idx, ...)
        
        # 动态阈值修剪
        if score < dynamic_threshold:  # 低于第5步最低20%的分数
            frozen_tokens.add(token_idx)  # 累积冻结
            
    return create_active_mask(frozen_tokens)

# 6. 渐进式效果（基于内容自适应）
# Layer 1-5:  100% token激活 (基准期，确定阈值)
# Layer 6-10: 90%+ token激活 (开始修剪最低分token)
# Layer 11-15: 70-85% token激活 (加速修剪)
# Layer 16-20: 50-70% token激活 (深度修剪)
# Layer 21+:   30-50% token激活 (最大修剪)
# 低噪声专家: 100% token激活 (处理完整信息)
```

**💡 最终改进的预期收益**：

**🎯 基准期 + 动态阈值优势**：
- **稳定基准**: 前5步完全推理，建立可靠的评分基准和统计信息
- **内容自适应**: 基于第5步实际分数分布确定阈值，适应不同内容复杂度
- **可控修剪**: percentile_threshold参数(如20%)直观控制修剪激进程度

**📊 量级对齐优势**：
- **趋势保留**: 避免0-1归一化掩盖分数整体下降趋势
- **量级统一**: attention分数缩放到变化分数量级，确保平衡权重
- **真实反映**: 保留分数的绝对大小信息，更准确反映token重要性变化

**⚡ 改进的渐进式修剪**：
- **平滑启动**: 5步基准期 → 逐步修剪，避免早期错误决策
- **累积智能**: 基于充分统计信息的动态阈值，比固定阈值更精确
- **自适应性**: 不同内容会有不同的修剪比例，而非机械的固定比例

**📈 新的修剪时间线**：
```
推理步骤 1-5:   100% token激活 (基准期，收集统计)
🎯 第5步结束:   确定动态阈值 = 第20百分位数分数
推理步骤 6-10:  开始修剪低于阈值的token
推理步骤 11-15: 更多token分数下降，修剪比例增加
推理步骤 16-20: 深度修剪阶段
推理步骤 21+:   最终修剪状态
专家切换后:     100% token激活 (低噪声专家)
```

**🔬 基于真实Latent变化的Token评分**：
- **变化幅度**: 基于真实的latent tensor差异计算
- **空间映射**: 将latent变化精确映射到patch级别的token
- **真实阈值**: 基于第5步实际变化分布的百分位数
- **零模拟**: 完全移除attention权重模拟，专注真实变化

**⚙️ 关键可调参数**：
```bash
--enable_token_pruning           # 启用Token裁剪
--pruning_threshold 20           # 百分位阈值 (10-30)
--pruning_baseline_steps 5       # 基准期长度 (3-8)
--pruning_start_layer 6          # 开始裁剪层数
--pruning_end_layer 999          # 结束层数 (自动调整到高噪声专家边界)
```

**🔥 真实计算节省验证**：
```python
# ✅ CFG截断: 真正减少50%模型调用
if is_final_steps:
    noise_pred = model(x, **kwargs_null)    # 1次调用
else:
    pred_cond = model(x, **kwargs_c)        # 2次调用
    pred_uncond = model(x, **kwargs_null)

# ✅ Token裁剪: 真正减少tensor计算
active_indices = torch.where(active_mask)[0]
x_active = x[:, active_indices, :]          # 减少张量大小
attention_out = self_attn(x_active)         # 减少attention计算
ffn_out = ffn(x_active)                     # 减少FFN计算

# ✅ 计算节省量化
attention_savings = 1 - (k/N)²              # Self-attention: O(N²) -> O(k²)
ffn_savings = 1 - (k/N)                     # FFN: O(N) -> O(k)
```

**🎯 实现难点**：
- 需要修改attention机制以支持部分token更新
- KV缓存管理复杂度增加
- 与FSDP/序列并行的兼容性
- 动态mask的梯度处理

**📊 实现优先级**: 中等到高
- 技术可行性强，但需要深度修改attention层
- 建议先在单GPU环境下原型验证
- 成功后再适配多GPU分布式环境

## 技术支持
如有问题，请参考：
1. 官方README.md文档
2. Hugging Face模型页面: https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B
3. 相关技术论文和文档

---
**再次提醒：此仓库仅用于代码研究，请勿在本地运行模型！**

