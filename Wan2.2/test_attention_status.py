#!/usr/bin/env python3
# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
"""
æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–çŠ¶æ€çš„è„šæœ¬
æ¼”ç¤ºå¦‚ä½•é€šè¿‡å‘½ä»¤è¡Œç¡®è®¤æ˜¯å¦å¯åŠ¨å¯è§†åŒ–
"""

import argparse
import torch
from wan.text2video import WanT2V


def test_attention_visualization_status():
    """æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–çŠ¶æ€"""
    
    print("=== WANæ¨¡å‹æ³¨æ„åŠ›å¯è§†åŒ–çŠ¶æ€æµ‹è¯• ===\n")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("æ­£åœ¨åŠ è½½WANæ¨¡å‹...")
    try:
        model = WanT2V.from_pretrained(
            "pkucaoyuan/WAN2.2_T2V_A14B",
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æç¤ºè¯
    test_prompt = "A beautiful sunset over the ocean"
    
    print("=" * 50)
    print("æµ‹è¯•1: ç¦ç”¨æ³¨æ„åŠ›å¯è§†åŒ–")
    print("=" * 50)
    
    try:
        video, timing_info = model.generate(
            input_prompt=test_prompt,
            frame_num=4,  # å¾ˆå°‘çš„å¸§æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
            size=(256, 256),
            sampling_steps=5,  # å¾ˆå°‘çš„æ­¥æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
            guide_scale=7.5,
            enable_attention_visualization=False,  # ç¦ç”¨å¯è§†åŒ–
            attention_output_dir="test_no_attention"
        )
        print("âœ… ç¦ç”¨å¯è§†åŒ–æµ‹è¯•å®Œæˆ")
        print(f"   è§†é¢‘å½¢çŠ¶: {video.shape}")
    except Exception as e:
        print(f"âŒ ç¦ç”¨å¯è§†åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: å¯ç”¨æ³¨æ„åŠ›å¯è§†åŒ–")
    print("=" * 50)
    
    try:
        video, timing_info = model.generate(
            input_prompt=test_prompt,
            frame_num=4,  # å¾ˆå°‘çš„å¸§æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
            size=(256, 256),
            sampling_steps=5,  # å¾ˆå°‘çš„æ­¥æ•°ç”¨äºå¿«é€Ÿæµ‹è¯•
            guide_scale=7.5,
            enable_attention_visualization=True,  # å¯ç”¨å¯è§†åŒ–
            attention_output_dir="test_with_attention"
        )
        print("âœ… å¯ç”¨å¯è§†åŒ–æµ‹è¯•å®Œæˆ")
        print(f"   è§†é¢‘å½¢çŠ¶: {video.shape}")
    except Exception as e:
        print(f"âŒ å¯ç”¨å¯è§†åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")
    print("è¯·æ£€æŸ¥ä»¥ä¸‹ç›®å½•:")
    print("- test_no_attention/: åº”è¯¥ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
    print("- test_with_attention/: åº”è¯¥åŒ…å« average_cross_attention_map.png")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æµ‹è¯•WANæ¨¡å‹æ³¨æ„åŠ›å¯è§†åŒ–çŠ¶æ€")
    parser.add_argument("--prompt", type=str, 
                       default="A beautiful sunset over the ocean",
                       help="æµ‹è¯•æç¤ºè¯")
    parser.add_argument("--frames", type=int, default=4, help="æµ‹è¯•å¸§æ•°")
    parser.add_argument("--steps", type=int, default=5, help="æµ‹è¯•æ­¥æ•°")
    parser.add_argument("--enable", action="store_true", help="å¯ç”¨æ³¨æ„åŠ›å¯è§†åŒ–")
    parser.add_argument("--disable", action="store_true", help="ç¦ç”¨æ³¨æ„åŠ›å¯è§†åŒ–")
    
    args = parser.parse_args()
    
    if args.enable and args.disable:
        print("âŒ ä¸èƒ½åŒæ—¶æŒ‡å®š --enable å’Œ --disable")
        return
    
    if args.enable or args.disable:
        # å•æ¬¡æµ‹è¯•
        print("=== WANæ¨¡å‹æ³¨æ„åŠ›å¯è§†åŒ–å•æ¬¡æµ‹è¯• ===\n")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åŠ è½½WANæ¨¡å‹...")
        try:
            model = WanT2V.from_pretrained(
                "pkucaoyuan/WAN2.2_T2V_A14B",
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return
        
        # ç¡®å®šå¯è§†åŒ–çŠ¶æ€
        enable_attention = args.enable
        output_dir = "test_attention_enabled" if enable_attention else "test_attention_disabled"
        
        print(f"ğŸ” æ³¨æ„åŠ›å¯è§†åŒ–: {'å¯ç”¨' if enable_attention else 'ç¦ç”¨'}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“ æç¤ºè¯: {args.prompt}")
        print(f"ğŸ¬ å¸§æ•°: {args.frames}")
        print(f"ğŸ”„ æ­¥æ•°: {args.steps}")
        print()
        
        try:
            video, timing_info = model.generate(
                input_prompt=args.prompt,
                frame_num=args.frames,
                size=(256, 256),
                sampling_steps=args.steps,
                guide_scale=7.5,
                enable_attention_visualization=enable_attention,
                attention_output_dir=output_dir
            )
            print("âœ… æµ‹è¯•å®Œæˆ")
            print(f"   è§†é¢‘å½¢çŠ¶: {video.shape}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        # å®Œæ•´æµ‹è¯•
        test_attention_visualization_status()


if __name__ == "__main__":
    main()
