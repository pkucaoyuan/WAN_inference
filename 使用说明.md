# WAN2.2-5B 模型使用说明

## 概述
这是WAN2.2-5B（Text-to-Image-to-Video）模型的代码仓库。此仓库**仅包含模型代码和配置文件**，不包含模型权重，以避免本地存储和运行大型模型。

## 目录结构
```
Wan2.2-TI2V-5B/
├── README.md                          # 官方说明文档
├── config.json                        # 模型配置文件
├── configuration.json                 # 配置信息
├── diffusion_pytorch_model.safetensors.index.json  # 权重索引文件
├── download_model_weights.py          # Python下载脚本
├── download_model_weights.bat         # Windows批处理下载脚本
├── 使用说明.md                        # 本文档
├── assets/                            # 资源文件夹
├── examples/                          # 示例文件
└── google/umt5-xxl/                   # T5编码器配置
```

## 下载模型权重

### 方法一：使用Python脚本（推荐）
```bash
python download_model_weights.py
```

### 方法二：使用Windows批处理脚本
```bash
download_model_weights.bat
```

### 方法三：手动使用Hugging Face CLI
```bash
# 安装依赖
pip install huggingface_hub[cli]

# 下载到指定目录
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./model_weights
```

### 方法四：使用Git LFS（如果在仓库目录中）
```bash
# 安装Git LFS
git lfs install

# 拉取LFS文件
git lfs pull
```

## 重要注意事项

### ⚠️ 本地运行限制
- **严禁在本地运行此模型**
- 模型文件非常大（约10GB+）
- 需要大量GPU内存和计算资源
- 建议仅在远程服务器或云平台上运行

### 🌐 推荐的远程运行平台
1. **启智算力平台** - 提供模型训练和推理云服务
2. **共绩算力** - 支持容器化部署，提供GPU资源
3. **Google Colab Pro** - 适合实验和测试
4. **AWS/Azure/GCP** - 企业级云计算平台

### 📋 系统要求（仅供参考，请勿本地运行）
- GPU: NVIDIA RTX 4090 或更高（24GB+ VRAM）
- RAM: 32GB+
- 存储: 50GB+ 可用空间
- CUDA: 11.8+
- Python: 3.8+

## 模型信息
- **模型名称**: WAN2.2-TI2V-5B
- **模型类型**: Text-to-Image-to-Video 生成模型
- **参数数量**: 50亿
- **输入**: 文本描述
- **输出**: 图像序列/视频

## 许可证
请参考原始仓库的许可证文件。

## 🚀 推理加速技巧

### **CFG截断优化（新功能）**
WAN2.2现在支持CFG截断技术，可以在最后几步跳过条件前传来加速推理：

**⚠️ 重要更新**: CFG截断现在**默认禁用**（cfg_truncate_steps=0），需要手动启用。

```bash
# 启用CFG截断（手动开启）
python generate.py --task ti2v-5B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --prompt "Your prompt here"

# 27B MOE模型双重CFG截断
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt here"
```

### **时间记录功能（新增）**
现在推理会自动记录详细的时间信息：
- ⏱️ **模型加载耗时**: 包含所有模型组件的加载时间
- 🔄 **专家切换总耗时**: MOE模型所有专家切换的累计时间
- ⚡ **纯推理耗时**: 扣除专家切换后的真实推理时间
- 📊 **总推理耗时**: 包含专家切换的完整推理时间
- 📈 **推理速度**: 步/秒（采样步骤执行速度）
- 🎬 **每步耗时**: 秒/步（单步平均时间）

### **帧数支持**
- ✅ **支持任意帧数**: 包括1帧生成
- 🔧 **移除4n+1限制**: 现在可以自由设置帧数
- 💡 **推荐值**: 虽然限制已移除，但81帧仍是最优配置

### **输出文件组织**
每次推理会自动创建结构化的输出文件夹：
```
outputs/
├── t2v-A14B_20250919_143052_A_beautiful_sunset/
│   ├── t2v-A14B_1280x720_20250919_143052.mp4    # 生成的视频
│   └── inference_record.json                     # 推理记录
├── t2v-A14B_20250919_143158_Another_prompt/
│   ├── t2v-A14B_1280x720_20250919_143158.mp4
│   └── inference_record.json
└── ...
```

**记录文件包含**：
- 📝 完整的参数设置
- ⏱️ 详细的性能数据  
- 🔧 分布式配置信息
- 📊 推理效率统计

### **双重CFG截断参数说明**
**低噪声专家截断（最后几步）**：
- `--cfg_truncate_steps 3`: 激进加速，轻微质量损失
- `--cfg_truncate_steps 5`: 平衡模式（推荐）
- `--cfg_truncate_steps 8`: 保守加速，质量优先  
- `--cfg_truncate_steps 0`: 禁用低噪声截断

**高噪声专家截断（专家切换前）**：
- `--cfg_truncate_high_noise_steps 2`: 轻微加速
- `--cfg_truncate_high_noise_steps 3`: 平衡模式（推荐）
- `--cfg_truncate_high_noise_steps 5`: 激进加速
- `--cfg_truncate_high_noise_steps 0`: 禁用高噪声截断

### **组合使用示例**
```bash
# 双重CFG截断（最大加速）
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 5 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"

# 只截断高噪声专家
python generate.py --task t2v-A14B \
    --cfg_truncate_steps 0 \
    --cfg_truncate_high_noise_steps 3 \
    --prompt "Your prompt"
```

### **快速加载优化**
```bash
# 启用快速加载模式（需要更多GPU内存）
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 1 \
    --prompt "Your prompt"
```

**快速加载功能**：
- 🚀 **并行加载**: 两个专家模型同时加载
- 🔥 **GPU预热**: 自动初始化CUDA环境
- 💾 **内存预分配**: 优化GPU内存管理
- 🚫 **禁用卸载**: 所有模型常驻GPU，避免切换开销

### **帧插值优化（新功能）**
```bash
# 启用帧插值模式
python generate.py --task t2v-A14B \
    --enable_frame_interpolation \
    --frame_num 80 \
    --prompt "Your prompt"
```

**帧插值功能**：
- 🎬 **智能分帧**: 高噪声专家只生成一半帧数（如80帧→40帧）
- 🔄 **自动插值**: 通过线性插值将帧数补齐到目标帧数
- ⚡ **显著加速**: 高噪声专家计算量减少50%
- 🎯 **质量保持**: 低噪声专家处理完整帧数，保持最终质量
- 📊 **灵活配置**: 自动调整奇数帧数为偶数，便于插值

**工作原理**：
1. **高噪声阶段**: 生成 `frame_num/2` 帧的latent表示
2. **帧插值**: 使用线性插值将帧数扩展到 `frame_num`
3. **低噪声阶段**: 处理完整的 `frame_num` 帧
4. **最终输出**: 生成目标帧数的视频

**性能提升**：
- **高噪声专家**: 50%计算量减少（生成一半帧数）
- **整体加速**: 预计20-30%总推理时间减少
- **内存节省**: 高噪声阶段内存使用减少50%
- **质量影响**: 最小，主要影响细节平滑度

## 🌐 **多机分布式推理**

### **多机推理架构**
WAN2.2支持跨多台机器的分布式推理，通过NCCL后端实现高效的跨节点通信：

```bash
# 多机推理配置 (2台机器，每台4GPU)
# 机器1 (主节点):
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=0 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8

# 机器2 (工作节点):  
MASTER_ADDR=192.168.1.100 MASTER_PORT=29500 \
WORLD_SIZE=8 NODE_RANK=1 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 \
    generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 8
```

### **环境变量说明**
- `MASTER_ADDR`: 主节点IP地址
- `MASTER_PORT`: 通信端口（通常29500）
- `WORLD_SIZE`: 总GPU数量（所有机器GPU总和）
- `NODE_RANK`: 当前机器编号（0为主节点）
- `--nnodes`: 机器总数
- `--nproc_per_node`: 每台机器的GPU数量

### **网络要求**
- **带宽**: 推荐10Gbps以上网络连接
- **延迟**: <1ms机器间延迟
- **端口**: 确保29500端口开放
- **防火墙**: 关闭或配置允许NCCL通信

### **性能提升**
- **单独低噪声截断**: 20-30%时间减少
- **单独高噪声截断**: 15-25%时间减少  
- **双重截断**: 35-50%时间减少
- **快速加载**: 50-70%加载时间减少
- **帧插值优化**: 20-30%总时间减少（高噪声专家50%加速）
- **多机分布式**: 接近线性扩展（2-4倍加速）
- **质量影响**: 最小，主要在细节层面
- **适用场景**: 所有WAN2.2 MOE模型（27B）

### **多机配置示例**

**2机8卡配置**:
```bash
# 总计算资源: 2台机器 × 4GPU = 8GPU
# 参数分片: 27B ÷ 8 = 3.375GB/GPU
# 序列并行: ulysses_size=8
# 预期加速: 6-7倍（相比单GPU）
```

**4机16卡配置**:
```bash
# 总计算资源: 4台机器 × 4GPU = 16GPU  
# 参数分片: 27B ÷ 16 = 1.6875GB/GPU
# 序列并行: ulysses_size=16
# 预期加速: 12-14倍（相比单GPU）
```

## 🔧 **常见问题和解决方案**

### **多GPU分布式错误处理**

**SIGSEGV (exitcode: -11) 和 SIGABRT (exitcode: -6) 错误**：
这些错误通常在多GPU FSDP环境中出现，主要由并发加载冲突引起。

**解决方案**：

```bash
# 方案1: 保守内存 + 线程优化
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
torchrun --nproc_per_node=4 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 4 \
    --frame_num 121

# 方案2: 降低GPU数量
torchrun --nproc_per_node=2 generate.py --task t2v-A14B \
    --dit_fsdp --t5_fsdp --ulysses_size 2 \
    --frame_num 121

# 方案3: 单GPU稳定模式
python generate.py --task t2v-A14B \
    --fast_loading \
    --frame_num 121

# 方案4: 启用模型卸载（最保守）
python generate.py --task t2v-A14B \
    --offload_model True --convert_model_dtype --t5_cpu \
    --frame_num 121
```

**错误发生时机**: 通常在FSDP模型分片初始化阶段，而不是推理阶段。

### **CFG截断实际效果分析**
基于实际测试，CFG截断的加速效果约为10-15%，主要原因：

- **专家切换占主导**: 101秒切换 vs 133秒推理
- **CFG截断只优化推理部分**: 无法减少切换开销  
- **内存访问瓶颈**: 单次前传节省被其他操作抵消

**更有效的加速策略**：
1. **使用--fast_loading**: 消除101秒专家切换时间
2. **多GPU分片**: 3-4倍整体加速
3. **CFG截断**: 在前两者基础上额外10-15%加速


## 技术支持
如有问题，请参考：
1. 官方README.md文档
2. Hugging Face模型页面: https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B
3. 相关技术论文和文档

---
**再次提醒：此仓库仅用于代码研究，请勿在本地运行模型！**

