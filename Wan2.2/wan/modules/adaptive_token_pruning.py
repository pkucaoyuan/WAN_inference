# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
"""
è‡ªé€‚åº”Tokenä¿®å‰ªæ¨¡å—
å®ç°åŸºäºå˜åŒ–å¹…åº¦çš„åŠ¨æ€tokenåœæ­¢æ›´æ–°æœºåˆ¶
"""

import torch
import torch.nn as nn
import math

class AdaptiveTokenPruning:
    """
    è‡ªé€‚åº”Tokenä¿®å‰ªå™¨
    åŸºäºtokenå˜åŒ–ã€è‡ªæ³¨æ„åŠ›æƒé‡ã€è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡çš„ç»¼åˆè¯„åˆ†è¿›è¡Œä¿®å‰ª
    """
    
    def __init__(self, 
                 baseline_steps=5,             # å‰å‡ æ­¥å®Œå…¨æ¨ç†ï¼Œç”¨äºå»ºç«‹åŸºå‡†
                 percentile_threshold=20,      # ç¬¬5æ­¥æœ€ä½x%ä½œä¸ºé˜ˆå€¼ (å¯è°ƒå‚æ•°)
                 start_layer=6,                # å¼€å§‹ä¿®å‰ªçš„å±‚æ•°ï¼ˆç¬¬6æ­¥å¼€å§‹ï¼‰
                 end_layer=35,                 # ç»“æŸä¿®å‰ªçš„å±‚æ•°ï¼ˆé«˜å™ªå£°ä¸“å®¶ç»“æŸå‰ï¼‰
                 change_weight=0.4,            # tokenå˜åŒ–æƒé‡
                 self_attn_weight=0.3,         # è‡ªæ³¨æ„åŠ›æƒé‡
                 cross_attn_weight=0.3,        # è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡
                 image_token_start=0,          # å›¾åƒtokenèµ·å§‹ç´¢å¼•
                 text_token_start=None,        # æ–‡æœ¬tokenèµ·å§‹ç´¢å¼•
                 expert_name="high_noise"):    # ä¸“å®¶åç§°ï¼Œåªåœ¨é«˜å™ªå£°ä¸“å®¶ä½¿ç”¨
        self.baseline_steps = baseline_steps
        self.percentile_threshold = percentile_threshold
        self.start_layer = start_layer
        self.end_layer = end_layer
        self.expert_name = expert_name
        
        # åŠ¨æ€é˜ˆå€¼ï¼ˆå°†åœ¨ç¬¬baseline_stepsæ­¥ç¡®å®šï¼‰
        self.dynamic_threshold = None
        
        # åŠ æƒç³»æ•°
        self.change_weight = change_weight
        self.self_attn_weight = self_attn_weight  
        self.cross_attn_weight = cross_attn_weight
        
        # Tokenåˆ†åŒº
        self.image_token_start = image_token_start
        self.text_token_start = text_token_start
        
        # çŠ¶æ€è¿½è¸ª
        self.token_states = {}
        self.frozen_tokens = set()
        self.token_scores_history = {}
        
        # å˜åŒ–åˆ†æ•°ç»Ÿè®¡ï¼ˆç”¨äºattentionåˆ†æ•°é‡çº§å¯¹é½ï¼‰
        self.change_score_stats = {
            'min': float('inf'), 
            'max': 0.0, 
            'sum': 0.0, 
            'count': 0,
            'values': []  # å­˜å‚¨æ‰€æœ‰å˜åŒ–åˆ†æ•°ç”¨äºè®¡ç®—ç™¾åˆ†ä½æ•°
        }
        
        # ç¬¬baseline_stepsæ­¥çš„æ‰€æœ‰tokenç»¼åˆè¯„åˆ†ï¼ˆç”¨äºç¡®å®šåŠ¨æ€é˜ˆå€¼ï¼‰
        self.baseline_scores = []
        
    def calculate_image_self_attention_score(self, self_attn_weights, token_idx, seq_len):
        """
        è®¡ç®—å•ä¸ªå›¾åƒtokenåœ¨è‡ªæ³¨æ„åŠ›ä¸­çš„æƒé‡æ€»å’Œ
        
        Args:
            self_attn_weights: [B, H, L, L] è‡ªæ³¨æ„åŠ›æƒé‡çŸ©é˜µ
            token_idx: ç›®æ ‡tokenç´¢å¼•
            seq_len: å›¾åƒtokenåºåˆ—é•¿åº¦
            
        Returns:
            float: è¯¥tokençš„è‡ªæ³¨æ„åŠ›æƒé‡æ€»å’Œ
        """
        if self.text_token_start is None:
            # å‡è®¾æ‰€æœ‰tokenéƒ½æ˜¯å›¾åƒtoken
            image_end = seq_len
        else:
            image_end = self.text_token_start
            
        # è®¡ç®—è¯¥tokenä¸æ‰€æœ‰å›¾åƒtokençš„æ³¨æ„åŠ›æƒé‡
        # token_idxä½œä¸ºqueryï¼Œä¸æ‰€æœ‰å›¾åƒtokenä½œä¸ºkeyçš„æ³¨æ„åŠ›æƒé‡
        image_self_attn = self_attn_weights[0, :, token_idx, self.image_token_start:image_end]
        
        # æ‰€æœ‰å¤´çš„å¹³å‡ï¼Œç„¶åæ±‚å’Œ
        score = image_self_attn.mean(dim=0).sum().item()
        return score
    
    def calculate_image_cross_attention_score(self, cross_attn_weights, token_idx):
        """
        è®¡ç®—å•ä¸ªå›¾åƒtokenä¸æ–‡æœ¬tokençš„è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡æ€»å’Œ
        
        Args:
            cross_attn_weights: [B, H, L, text_len] è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
            token_idx: ç›®æ ‡å›¾åƒtokenç´¢å¼•
            
        Returns:
            float: è¯¥tokenä¸æ–‡æœ¬tokençš„æ³¨æ„åŠ›æƒé‡æ€»å’Œ
        """
        if cross_attn_weights is None:
            return 0.0
            
        # è®¡ç®—è¯¥å›¾åƒtokenå¯¹æ‰€æœ‰æ–‡æœ¬tokençš„æ³¨æ„åŠ›æƒé‡
        cross_attn = cross_attn_weights[0, :, token_idx, :]  # [H, text_len]
        
        # æ‰€æœ‰å¤´çš„å¹³å‡ï¼Œç„¶åæ±‚å’Œ
        score = cross_attn.mean(dim=0).sum().item()
        return score
    
    def update_change_score_statistics(self, change_score):
        """æ›´æ–°å˜åŒ–åˆ†æ•°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.change_score_stats
        stats['min'] = min(stats['min'], change_score)
        stats['max'] = max(stats['max'], change_score)
        stats['sum'] += change_score
        stats['count'] += 1
        stats['values'].append(change_score)
    
    def scale_attention_scores_to_change_magnitude(self, self_attn_score, cross_attn_score):
        """å°†attentionåˆ†æ•°ç¼©æ”¾åˆ°å˜åŒ–åˆ†æ•°çš„é‡çº§"""
        if self.change_score_stats['count'] == 0:
            return self_attn_score, cross_attn_score
            
        # ä½¿ç”¨å˜åŒ–åˆ†æ•°çš„å¹³å‡å€¼ä½œä¸ºç›®æ ‡é‡çº§
        change_avg = self.change_score_stats['sum'] / self.change_score_stats['count']
        change_range = self.change_score_stats['max'] - self.change_score_stats['min']
        
        if change_range == 0:
            return self_attn_score, cross_attn_score
        
        # å°†attentionåˆ†æ•°ç¼©æ”¾åˆ°å˜åŒ–åˆ†æ•°çš„é‡çº§
        # å‡è®¾attentionåˆ†æ•°çš„åˆç†èŒƒå›´ï¼Œç„¶åçº¿æ€§ç¼©æ”¾åˆ°changeåˆ†æ•°èŒƒå›´
        scaled_self_attn = self_attn_score * (change_range / max(self_attn_score, 1e-8))
        scaled_cross_attn = cross_attn_score * (change_range / max(cross_attn_score, 1e-8))
        
        return scaled_self_attn, scaled_cross_attn

    def calculate_composite_score(self, token_idx, current_hidden, prev_hidden, 
                                self_attn_weights, cross_attn_weights, seq_len, layer_idx):
        """
        è®¡ç®—åŸºäºé‡çº§å¯¹é½çš„ç»¼åˆè¯„åˆ†ï¼šå˜åŒ–å€¼ + ç¼©æ”¾åçš„attentionæƒé‡
        
        Args:
            token_idx: tokenç´¢å¼•
            current_hidden: å½“å‰éšè—çŠ¶æ€
            prev_hidden: å‰ä¸€å±‚éšè—çŠ¶æ€
            self_attn_weights: è‡ªæ³¨æ„åŠ›æƒé‡
            cross_attn_weights: è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡
            seq_len: åºåˆ—é•¿åº¦
            layer_idx: å½“å‰å±‚ç´¢å¼•
            
        Returns:
            tuple: (ç»¼åˆè¯„åˆ†, åŸå§‹åˆ†æ•°å­—å…¸, ç¼©æ”¾ååˆ†æ•°å­—å…¸)
        """
        raw_scores = {}
        
        # 1. Tokenå˜åŒ–åˆ†æ•°ï¼ˆåŸºå‡†é‡çº§ï¼‰
        if prev_hidden is not None:
            change_magnitude = torch.norm(current_hidden - prev_hidden, dim=-1).item()
            relative_change = change_magnitude / (torch.norm(prev_hidden, dim=-1).item() + 1e-8)
            raw_scores['change'] = relative_change
        else:
            raw_scores['change'] = 1.0  # ç¬¬ä¸€å±‚é»˜è®¤é«˜é‡è¦æ€§
        
        # 2. è‡ªæ³¨æ„åŠ›åˆ†æ•°ï¼ˆå›¾åƒtokené—´çš„é‡è¦æ€§ï¼‰
        raw_scores['self_attn'] = self.calculate_image_self_attention_score(
            self_attn_weights, token_idx, seq_len)
        
        # 3. è·¨æ¨¡æ€æ³¨æ„åŠ›åˆ†æ•°ï¼ˆä¸æ–‡æœ¬çš„å…³è”æ€§ï¼‰
        raw_scores['cross_attn'] = self.calculate_image_cross_attention_score(
            cross_attn_weights, token_idx)
        
        # 4. æ›´æ–°å˜åŒ–åˆ†æ•°ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‰baseline_stepsæ­¥æ”¶é›†ç»Ÿè®¡ï¼‰
        if layer_idx <= self.baseline_steps:
            self.update_change_score_statistics(raw_scores['change'])
        
        # 5. å°†attentionåˆ†æ•°ç¼©æ”¾åˆ°å˜åŒ–åˆ†æ•°çš„é‡çº§
        scaled_self_attn, scaled_cross_attn = self.scale_attention_scores_to_change_magnitude(
            raw_scores['self_attn'], raw_scores['cross_attn'])
        
        scaled_scores = {
            'change': raw_scores['change'],
            'self_attn': scaled_self_attn,
            'cross_attn': scaled_cross_attn
        }
        
        # 6. åŠ æƒç»¼åˆè¯„åˆ†ï¼ˆé‡çº§å¯¹é½åçš„åˆ†æ•°ï¼‰
        composite_score = (
            self.change_weight * scaled_scores['change'] +
            self.self_attn_weight * scaled_scores['self_attn'] + 
            self.cross_attn_weight * scaled_scores['cross_attn']
        )
        
        return composite_score, raw_scores, scaled_scores

    def calculate_dynamic_threshold(self):
        """æ ¹æ®ç¬¬baseline_stepsæ­¥çš„åˆ†æ•°åˆ†å¸ƒè®¡ç®—åŠ¨æ€é˜ˆå€¼"""
        if len(self.baseline_scores) == 0:
            return None
            
        # è®¡ç®—ç¬¬percentile_thresholdç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        import numpy as np
        threshold = np.percentile(self.baseline_scores, self.percentile_threshold)
        return threshold

    def should_apply_pruning(self, layer_idx, expert_name):
        """
        åˆ¤æ–­å½“å‰å±‚æ˜¯å¦åº”è¯¥åº”ç”¨tokenä¿®å‰ª
        
        Args:
            layer_idx: å½“å‰å±‚ç´¢å¼•
            expert_name: ä¸“å®¶åç§° ("high_noise" æˆ– "low_noise")
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥åº”ç”¨ä¿®å‰ª
        """
        # 1. åªåœ¨é«˜å™ªå£°ä¸“å®¶ä¸­åº”ç”¨
        if expert_name != "high_noise":
            return False
            
        # 2. å‰baseline_stepsæ­¥å®Œå…¨æ¨ç†ï¼Œä¸ä¿®å‰ª
        if layer_idx <= self.baseline_steps:
            return False
            
        # 3. åªåœ¨æŒ‡å®šå±‚æ•°èŒƒå›´å†…åº”ç”¨ï¼ˆæ¸è¿›å¼ä¿®å‰ªï¼‰
        if layer_idx > self.end_layer:
            return False
            
        return True

    def should_prune_token(self, token_idx, composite_score, layer_idx):
        """
        åŸºäºåŠ¨æ€é˜ˆå€¼åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿®å‰ªæŸä¸ªtoken
        
        Args:
            token_idx: tokenç´¢å¼•
            composite_score: ç»¼åˆè¯„åˆ†
            layer_idx: å½“å‰å±‚ç´¢å¼•
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥ä¿®å‰ªè¯¥token
        """
        # å·²ç»å†»ç»“çš„tokenä¿æŒå†»ç»“
        if token_idx in self.frozen_tokens:
            return True
        
        # åªå¯¹å›¾åƒtokenè¿›è¡Œä¿®å‰ªåˆ¤æ–­
        if self.text_token_start is not None and token_idx >= self.text_token_start:
            return False  # æ–‡æœ¬tokenä¸ä¿®å‰ª
            
        # å¦‚æœè¿˜æ²¡æœ‰ç¡®å®šåŠ¨æ€é˜ˆå€¼ï¼Œä¸ä¿®å‰ª
        if self.dynamic_threshold is None:
            return False
            
        # åŠ¨æ€é˜ˆå€¼ä¿®å‰ªï¼šè¯„åˆ†ä½äºç¬¬baseline_stepsæ­¥æœ€ä½x%çš„é˜ˆå€¼å°±ä¿®å‰ª
        return composite_score < self.dynamic_threshold
    
    def identify_structural_tokens(self, attention_weights, seq_len):
        """
        è¯†åˆ«ç»“æ„æ€§tokenï¼ˆé«˜æ³¨æ„åŠ›æƒé‡çš„tokenï¼‰
        
        Args:
            attention_weights: [B, H, L, L] æ³¨æ„åŠ›æƒé‡
            seq_len: åºåˆ—é•¿åº¦
            
        Returns:
            set: ç»“æ„tokençš„ç´¢å¼•é›†åˆ
        """
        # è®¡ç®—æ¯ä¸ªtokençš„å¹³å‡æ³¨æ„åŠ›æ¥æ”¶é‡
        avg_attention = attention_weights.mean(dim=(0, 1))  # [L, L]
        token_importance = avg_attention.sum(dim=-1)  # [L]
        
        # é€‰æ‹©top-kä½œä¸ºç»“æ„token
        num_structural = int(seq_len * self.structural_ratio)
        structural_indices = torch.topk(token_importance, num_structural).indices
        
        return set(structural_indices.cpu().tolist())
    
    def apply_progressive_pruning(self, hidden_states, self_attn_weights, cross_attn_weights, 
                                layer_idx, expert_name):
        """
        åº”ç”¨æ¸è¿›å¼tokenä¿®å‰ªï¼ˆåªåœ¨é«˜å™ªå£°ä¸“å®¶ä¸­ä½¿ç”¨ï¼‰
        
        Args:
            hidden_states: [B, L, C] å½“å‰hidden states
            self_attn_weights: [B, H, L, L] è‡ªæ³¨æ„åŠ›æƒé‡
            cross_attn_weights: [B, H, L, text_len] è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡
            layer_idx: å½“å‰å±‚ç´¢å¼•
            expert_name: ä¸“å®¶åç§°
            
        Returns:
            tuple: (ä¿®å‰ªåçš„hidden_states, æ¿€æ´»mask, ä¿®å‰ªç»Ÿè®¡)
        """
        B, L, C = hidden_states.shape
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åº”ç”¨ä¿®å‰ª
        if not self.should_apply_pruning(layer_idx, expert_name):
            # ä¸ä¿®å‰ªï¼Œè¿”å›å…¨æ¿€æ´»mask
            active_mask = torch.ones(L, dtype=torch.bool, device=hidden_states.device)
            pruning_stats = {
                'layer': layer_idx,
                'expert': expert_name,
                'pruning_applied': False,
                'reason': 'outside_pruning_range' if expert_name == "high_noise" else 'low_noise_expert'
            }
            return hidden_states, active_mask, pruning_stats
        
        # åˆ›å»ºæ¿€æ´»mask
        active_mask = torch.ones(L, dtype=torch.bool, device=hidden_states.device)
        
        # è·å–å‰ä¸€å±‚çŠ¶æ€ç”¨äºæ¯”è¾ƒ
        prev_hidden = self.token_states.get(layer_idx - 1)
        
        # è®¡ç®—æ‰€æœ‰å›¾åƒtokençš„ç»¼åˆè¯„åˆ†
        token_scores = []
        raw_scores_list = []
        normalized_scores_list = []
        
        image_token_end = self.text_token_start if self.text_token_start is not None else L
        
        for token_idx in range(self.image_token_start, image_token_end):
            composite_score, raw_scores, normalized_scores = self.calculate_composite_score(
                token_idx, 
                hidden_states[0, token_idx],
                prev_hidden[0, token_idx] if prev_hidden is not None else None,
                self_attn_weights, cross_attn_weights, L, layer_idx
            )
            token_scores.append((token_idx, composite_score))
            raw_scores_list.append(raw_scores)
            normalized_scores_list.append(normalized_scores)
        
        # åœ¨ç¬¬baseline_stepsæ­¥æ”¶é›†æ‰€æœ‰åˆ†æ•°ç”¨äºç¡®å®šåŠ¨æ€é˜ˆå€¼
        if layer_idx == self.baseline_steps:
            self.baseline_scores = [score for _, score in token_scores]
            self.dynamic_threshold = self.calculate_dynamic_threshold()
            print(f"ğŸ¯ åŠ¨æ€é˜ˆå€¼å·²ç¡®å®š: {self.dynamic_threshold:.4f} (ç¬¬{self.percentile_threshold}ç™¾åˆ†ä½æ•°)")
        
        # æ¸è¿›å¼ä¿®å‰ªï¼šåŸºäºåŠ¨æ€é˜ˆå€¼ï¼Œé€æ­¥å¢åŠ ä¿®å‰ªçš„token
        newly_frozen = []
        for i, (token_idx, score) in enumerate(token_scores):
            if self.should_prune_token(token_idx, score, layer_idx):
                active_mask[token_idx] = False
                if token_idx not in self.frozen_tokens:
                    self.frozen_tokens.add(token_idx)
                    newly_frozen.append(token_idx)
        
        # è®°å½•è¯¦ç»†çš„è¯„åˆ†å†å²
        for i, (token_idx, score) in enumerate(token_scores):
            if token_idx not in self.token_scores_history:
                self.token_scores_history[token_idx] = []
            self.token_scores_history[token_idx].append({
                'layer': layer_idx,
                'expert': expert_name,
                'composite_score': score,
                'is_active': active_mask[token_idx].item(),
                'raw_scores': raw_scores_list[i],
                'normalized_scores': normalized_scores_list[i]
            })
        
        # ä¿å­˜å½“å‰çŠ¶æ€
        self.token_states[layer_idx] = hidden_states.clone()
        
        # ä¿®å‰ªç»Ÿè®¡
        total_image_tokens = image_token_end - self.image_token_start
        active_image_tokens = active_mask[self.image_token_start:image_token_end].sum().item()
        
        pruning_stats = {
            'layer': layer_idx,
            'expert': expert_name,
            'pruning_applied': True,
            'total_tokens': L,
            'image_tokens': total_image_tokens,
            'active_image_tokens': active_image_tokens,
            'pruned_image_tokens': total_image_tokens - active_image_tokens,
            'newly_frozen': len(newly_frozen),
            'cumulative_frozen': len(self.frozen_tokens),
            'image_pruning_ratio': (total_image_tokens - active_image_tokens) / total_image_tokens,
            'dynamic_threshold': self.dynamic_threshold,
            'percentile_threshold': self.percentile_threshold,
            'avg_composite_score': sum(score for _, score in token_scores) / len(token_scores),
            'change_score_stats': dict(self.change_score_stats)  # å½“å‰å˜åŒ–åˆ†æ•°ç»Ÿè®¡
        }
        
        return hidden_states, active_mask, pruning_stats
    
    def get_pruning_summary(self):
        """è·å–ä¿®å‰ªè¿‡ç¨‹çš„è¯¦ç»†æ€»ç»“"""
        summary = {
            'total_frozen_tokens': len(self.frozen_tokens),
            'frozen_token_list': list(self.frozen_tokens),
            'scores_history': self.token_scores_history,
            'dynamic_threshold': self.dynamic_threshold,
            'percentile_threshold': self.percentile_threshold,
            'baseline_steps': self.baseline_steps,
            'change_score_stats': dict(self.change_score_stats)
        }
        return summary
    
    def save_pruning_log(self, output_dir, step_idx=None):
        """ä¿å­˜è£å‰ªæ—¥å¿—åˆ°è¾“å‡ºæ–‡ä»¶å¤¹"""
        import os
        import json
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if step_idx is not None:
            log_filename = f"token_pruning_step_{step_idx:02d}_{timestamp}.json"
        else:
            log_filename = f"token_pruning_summary_{timestamp}.json"
        
        log_path = os.path.join(output_dir, log_filename)
        
        # å‡†å¤‡æ—¥å¿—æ•°æ®
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'step_index': step_idx,
            'pruning_summary': self.get_pruning_summary(),
            'expert_name': self.expert_name,
            'configuration': {
                'baseline_steps': self.baseline_steps,
                'percentile_threshold': self.percentile_threshold,
                'change_weight': self.change_weight,
                'self_attn_weight': self.self_attn_weight,
                'cross_attn_weight': self.cross_attn_weight,
                'start_layer': self.start_layer,
                'end_layer': self.end_layer
            }
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        return log_path
    
    def save_step_pruning_stats(self, output_dir, step_idx, pruning_stats):
        """ä¿å­˜æ¯ä¸€æ­¥çš„è¯¦ç»†è£å‰ªç»Ÿè®¡ä¿¡æ¯"""
        import os
        import json
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ­¥éª¤ç»Ÿè®¡æ–‡ä»¶å
        stats_filename = f"pruning_step_{step_idx:02d}_stats.json"
        stats_path = os.path.join(output_dir, stats_filename)
        
        # è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        detailed_stats = {
            'step_index': step_idx,
            'timestamp': datetime.now().isoformat(),
            'expert_name': pruning_stats.get('expert', 'unknown'),
            'pruning_applied': pruning_stats.get('pruning_applied', False),
            
            # åŸºæœ¬ç»Ÿè®¡
            'token_statistics': {
                'total_tokens': pruning_stats.get('total_tokens', 0),
                'image_tokens': pruning_stats.get('image_tokens', 0),
                'active_image_tokens': pruning_stats.get('active_image_tokens', 0),
                'pruned_image_tokens': pruning_stats.get('pruned_image_tokens', 0),
                'newly_frozen': pruning_stats.get('newly_frozen', 0),
                'cumulative_frozen': pruning_stats.get('cumulative_frozen', 0)
            },
            
            # è£å‰ªæ¯”ä¾‹
            'pruning_ratios': {
                'image_pruning_ratio': pruning_stats.get('image_pruning_ratio', 0.0),
                'active_ratio': pruning_stats.get('active_image_tokens', 0) / max(pruning_stats.get('image_tokens', 1), 1),
                'cumulative_pruning_ratio': pruning_stats.get('cumulative_frozen', 0) / max(pruning_stats.get('image_tokens', 1), 1)
            },
            
            # é˜ˆå€¼å’Œè¯„åˆ†ä¿¡æ¯
            'threshold_info': {
                'dynamic_threshold': pruning_stats.get('dynamic_threshold'),
                'percentile_threshold': pruning_stats.get('percentile_threshold'),
                'avg_composite_score': pruning_stats.get('avg_composite_score', 0.0)
            },
            
            # å˜åŒ–åˆ†æ•°ç»Ÿè®¡
            'change_score_stats': pruning_stats.get('change_score_stats', {})
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_stats, f, indent=2, ensure_ascii=False)
        
        # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        if detailed_stats['pruning_applied']:
            print(f"ğŸ“Š Step {step_idx} è£å‰ªç»Ÿè®¡:")
            print(f"   æ¿€æ´»Token: {detailed_stats['token_statistics']['active_image_tokens']}/{detailed_stats['token_statistics']['image_tokens']} "
                  f"({detailed_stats['pruning_ratios']['active_ratio']:.1%})")
            print(f"   ç´¯ç§¯è£å‰ª: {detailed_stats['token_statistics']['cumulative_frozen']} "
                  f"({detailed_stats['pruning_ratios']['cumulative_pruning_ratio']:.1%})")
            print(f"   æœ¬æ­¥æ–°å¢è£å‰ª: {detailed_stats['token_statistics']['newly_frozen']}")
            if detailed_stats['threshold_info']['dynamic_threshold']:
                print(f"   åŠ¨æ€é˜ˆå€¼: {detailed_stats['threshold_info']['dynamic_threshold']:.4f}")
        
        return stats_path
    
    def generate_pruning_summary_report(self, output_dir):
        """ç”Ÿæˆå®Œæ•´çš„è£å‰ªè¿‡ç¨‹æ±‡æ€»æŠ¥å‘Š"""
        import os
        import json
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"token_pruning_summary_report_{timestamp}.txt"
        report_path = os.path.join(output_dir, report_filename)
        
        # å‡†å¤‡æ±‡æ€»æ•°æ®
        summary = self.get_pruning_summary()
        
        # åˆ†ætokenè¯„åˆ†å†å²ï¼ŒæŒ‰æ­¥éª¤ç»Ÿè®¡
        step_stats = {}
        for token_idx, history in summary['scores_history'].items():
            for record in history:
                step = record['layer']
                if step not in step_stats:
                    step_stats[step] = {
                        'active_tokens': 0,
                        'frozen_tokens': 0,
                        'total_tokens': 0,
                        'avg_score': 0.0,
                        'score_sum': 0.0
                    }
                
                step_stats[step]['total_tokens'] += 1
                step_stats[step]['score_sum'] += record['composite_score']
                
                if record['is_active']:
                    step_stats[step]['active_tokens'] += 1
                else:
                    step_stats[step]['frozen_tokens'] += 1
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        for step in step_stats:
            if step_stats[step]['total_tokens'] > 0:
                step_stats[step]['avg_score'] = step_stats[step]['score_sum'] / step_stats[step]['total_tokens']
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("WAN2.2 Tokenä¿®å‰ªè¿‡ç¨‹è¯¦ç»†æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ä¸“å®¶ç±»å‹: {self.expert_name}\n")
            f.write("\n")
            
            # é…ç½®ä¿¡æ¯
            f.write("ğŸ“‹ é…ç½®å‚æ•°:\n")
            f.write("-" * 40 + "\n")
            f.write(f"åŸºå‡†æ­¥æ•°: {self.baseline_steps}\n")
            f.write(f"ç™¾åˆ†ä½é˜ˆå€¼: {self.percentile_threshold}%\n")
            f.write(f"åŠ¨æ€é˜ˆå€¼: {self.dynamic_threshold:.4f}\n" if self.dynamic_threshold else "åŠ¨æ€é˜ˆå€¼: æœªç¡®å®š\n")
            f.write(f"å¼€å§‹å±‚æ•°: {self.start_layer}\n")
            f.write(f"ç»“æŸå±‚æ•°: {self.end_layer}\n")
            f.write(f"æƒé‡é…ç½®: å˜åŒ–({self.change_weight:.1f}) + è‡ªæ³¨æ„åŠ›({self.self_attn_weight:.1f}) + è·¨æ¨¡æ€({self.cross_attn_weight:.1f})\n")
            f.write("\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡:\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»å†»ç»“Tokenæ•°: {summary['total_frozen_tokens']}\n")
            f.write(f"å˜åŒ–åˆ†æ•°ç»Ÿè®¡: min={summary['change_score_stats'].get('min', 0):.4f}, "
                   f"max={summary['change_score_stats'].get('max', 0):.4f}, "
                   f"avg={summary['change_score_stats'].get('sum', 0) / max(summary['change_score_stats'].get('count', 1), 1):.4f}\n")
            f.write("\n")
            
            # æŒ‰æ­¥éª¤è¯¦ç»†ç»Ÿè®¡
            f.write("ğŸ“ˆ åˆ†æ­¥éª¤ç»Ÿè®¡:\n")
            f.write("-" * 80 + "\n")
            f.write(f"{'æ­¥éª¤':<6} {'æ€»Token':<8} {'æ¿€æ´»':<8} {'å†»ç»“':<8} {'æ¿€æ´»ç‡':<10} {'å¹³å‡åˆ†æ•°':<10}\n")
            f.write("-" * 80 + "\n")
            
            for step in sorted(step_stats.keys()):
                stats = step_stats[step]
                active_ratio = stats['active_tokens'] / max(stats['total_tokens'], 1) * 100
                f.write(f"{step:<6} {stats['total_tokens']:<8} {stats['active_tokens']:<8} "
                       f"{stats['frozen_tokens']:<8} {active_ratio:<9.1f}% {stats['avg_score']:<10.4f}\n")
            
            f.write("\n")
            
            # å†»ç»“Tokenåˆ—è¡¨
            f.write("ğŸ§Š å†»ç»“Tokenåˆ—è¡¨:\n")
            f.write("-" * 40 + "\n")
            frozen_tokens = sorted(summary['frozen_token_list'])
            for i, token_idx in enumerate(frozen_tokens):
                if i % 10 == 0 and i > 0:
                    f.write("\n")
                f.write(f"{token_idx:4d} ")
            f.write(f"\n\nå…± {len(frozen_tokens)} ä¸ªTokenè¢«å†»ç»“\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
            f.write("=" * 80 + "\n")
        
        print(f"ğŸ“„ è£å‰ªæ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path

def create_pruned_attention_mask(active_mask, original_mask=None):
    """
    åˆ›å»ºä¿®å‰ªåçš„æ³¨æ„åŠ›mask
    
    Args:
        active_mask: [L] æ¿€æ´»tokençš„mask
        original_mask: åŸå§‹æ³¨æ„åŠ›mask
        
    Returns:
        torch.Tensor: ä¿®å‰ªåçš„æ³¨æ„åŠ›mask
    """
    L = len(active_mask)
    
    # åˆ›å»ºåŸºç¡€maskï¼šå†»ç»“tokenåªèƒ½è¢«attentionï¼Œä¸èƒ½attend to others
    pruned_mask = torch.ones(L, L, dtype=torch.bool)
    
    # å†»ç»“tokençš„è¡Œè®¾ä¸ºFalseï¼ˆä¸èƒ½attend to othersï¼‰
    frozen_indices = (~active_mask).nonzero().flatten()
    if len(frozen_indices) > 0:
        pruned_mask[frozen_indices, :] = False
        # ä½†å…è®¸å…¶ä»–token attend to å†»ç»“tokenï¼ˆä¿æŒKVå¯ç”¨ï¼‰
    
    if original_mask is not None:
        pruned_mask = pruned_mask & original_mask
        
    return pruned_mask
