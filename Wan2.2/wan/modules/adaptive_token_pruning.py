"""
è‡ªé€‚åº”Tokenä¿®å‰ªæ¨¡å— - ç®€åŒ–ç‰ˆæœ¬
åªåŸºäºçœŸå®çš„latentå˜åŒ–è¿›è¡Œtokenä¿®å‰ªï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿæˆåˆ†
"""

import torch
import torch.nn as nn
import math

class AdaptiveTokenPruning:
    """
    è‡ªé€‚åº”Tokenä¿®å‰ªå™¨ - ç®€åŒ–ç‰ˆæœ¬
    ä»…åŸºäºçœŸå®çš„latentå˜åŒ–è¿›è¡Œä¿®å‰ªï¼Œç¡®ä¿100%çœŸå®è®¡ç®—èŠ‚çœ
    """
    
    def __init__(self, 
                 baseline_steps=5,             # å‰å‡ æ­¥å®Œå…¨æ¨ç†ï¼Œç”¨äºå»ºç«‹åŸºå‡†
                 percentile_threshold=20,      # ç¬¬5æ­¥æœ€ä½x%ä½œä¸ºé˜ˆå€¼ (å¯è°ƒå‚æ•°)
                 start_layer=6,                # å¼€å§‹ä¿®å‰ªçš„å±‚æ•°ï¼ˆç¬¬6æ­¥å¼€å§‹ï¼‰
                 end_layer=35,                 # ç»“æŸä¿®å‰ªçš„å±‚æ•°ï¼ˆé«˜å™ªå£°ä¸“å®¶ç»“æŸå‰ï¼‰
                 expert_name="high_noise"):    # ä¸“å®¶åç§°ï¼Œåªåœ¨é«˜å™ªå£°ä¸“å®¶ä½¿ç”¨
        self.baseline_steps = baseline_steps
        self.percentile_threshold = percentile_threshold
        self.start_layer = start_layer
        self.end_layer = end_layer
        self.expert_name = expert_name
        
        # åŠ¨æ€é˜ˆå€¼ï¼ˆå°†åœ¨ç¬¬baseline_stepsæ­¥ç¡®å®šï¼‰
        self.dynamic_threshold = None
        
        # çŠ¶æ€è¿½è¸ª
        self.frozen_tokens = set()
        
        # çœŸå®å˜åŒ–åˆ†æ•°ç»Ÿè®¡ï¼ˆç”¨äºåŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼‰
        self.change_score_stats = {
            'min': float('inf'), 
            'max': 0.0, 
            'sum': 0.0, 
            'count': 0,
            'values': []  # ä»…åœ¨è®¡ç®—é˜ˆå€¼æ—¶ä¸´æ—¶å­˜å‚¨ï¼Œä¸è¾“å‡ºåˆ°æ–‡ä»¶
        }
        
        # ç¬¬baseline_stepsæ­¥çš„æ‰€æœ‰tokençœŸå®å˜åŒ–è¯„åˆ†ï¼ˆç”¨äºç¡®å®šåŠ¨æ€é˜ˆå€¼ï¼‰
        self.baseline_scores = []
        
        # æ¯æ­¥æ—¶é—´è®°å½•
        self.step_timings = []
        
    def update_change_score_statistics(self, change_score):
        """æ›´æ–°çœŸå®å˜åŒ–åˆ†æ•°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.change_score_stats
        stats['min'] = min(stats['min'], change_score)
        stats['max'] = max(stats['max'], change_score)
        stats['sum'] += change_score
        stats['count'] += 1
        stats['values'].append(change_score)
    
    def calculate_dynamic_threshold(self):
        """æ ¹æ®ç¬¬baseline_stepsæ­¥çš„çœŸå®å˜åŒ–åˆ†æ•°åˆ†å¸ƒè®¡ç®—åŠ¨æ€é˜ˆå€¼"""
        if len(self.baseline_scores) == 0:
            return None
            
        # è®¡ç®—ç¬¬percentile_thresholdç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        import numpy as np
        threshold = np.percentile(self.baseline_scores, self.percentile_threshold)
        return threshold

    def should_apply_pruning(self, layer_idx, expert_name):
        """
        åˆ¤æ–­å½“å‰å±‚æ˜¯å¦åº”è¯¥åº”ç”¨tokenä¿®å‰ª
        
        Args:
            layer_idx: å½“å‰å±‚ç´¢å¼•
            expert_name: ä¸“å®¶åç§° ("high_noise" æˆ– "low_noise")
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥åº”ç”¨ä¿®å‰ª
        """
        # 1. åªåœ¨é«˜å™ªå£°ä¸“å®¶ä¸­åº”ç”¨
        if expert_name != "high_noise":
            return False
            
        # 2. å‰baseline_stepsæ­¥å®Œå…¨æ¨ç†ï¼Œä¸ä¿®å‰ª
        if layer_idx < self.baseline_steps:
            return False
            
        # 3. åªåœ¨æŒ‡å®šå±‚æ•°èŒƒå›´å†…åº”ç”¨ï¼ˆæ¸è¿›å¼ä¿®å‰ªï¼‰
        if layer_idx > self.end_layer:
            return False
            
        return True

    def should_prune_token(self, change_score):
        """
        åŸºäºçœŸå®å˜åŒ–åˆ†æ•°å’ŒåŠ¨æ€é˜ˆå€¼åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿®å‰ªæŸä¸ªtoken
        
        Args:
            change_score: çœŸå®çš„tokenå˜åŒ–åˆ†æ•°
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥ä¿®å‰ªè¯¥token
        """
        # å¦‚æœè¿˜æ²¡æœ‰ç¡®å®šåŠ¨æ€é˜ˆå€¼ï¼Œä¸ä¿®å‰ª
        if self.dynamic_threshold is None:
            return False
            
        # åŠ¨æ€é˜ˆå€¼ä¿®å‰ªï¼šå˜åŒ–åˆ†æ•°ä½äºé˜ˆå€¼å°±ä¿®å‰ª
        return change_score < self.dynamic_threshold
    
    def get_pruning_summary(self):
        """è·å–ä¿®å‰ªè¿‡ç¨‹çš„è¯¦ç»†æ€»ç»“"""
        # åˆ›å»ºä¸åŒ…å«è¯¦ç»†scoreå€¼çš„ç»Ÿè®¡ä¿¡æ¯
        stats_summary = {
            'min': self.change_score_stats.get('min', 0),
            'max': self.change_score_stats.get('max', 0),
            'sum': self.change_score_stats.get('sum', 0),
            'count': self.change_score_stats.get('count', 0),
            'avg': self.change_score_stats.get('sum', 0) / max(self.change_score_stats.get('count', 1), 1)
            # ä¸åŒ…å«'values'æ•°ç»„ï¼Œé¿å…è¾“å‡º3600ä¸ªscoreå€¼
        }
        
        summary = {
            'total_frozen_tokens': len(self.frozen_tokens),
            'frozen_token_list': list(self.frozen_tokens),
            'dynamic_threshold': self.dynamic_threshold,
            'percentile_threshold': self.percentile_threshold,
            'baseline_steps': self.baseline_steps,
            'change_score_stats': stats_summary  # åªåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸åŒ…å«åŸå§‹æ•°æ®
        }
        return summary
    
    def save_pruning_log(self, output_dir, step_idx=None):
        """ä¿å­˜è£å‰ªæ—¥å¿—åˆ°è¾“å‡ºæ–‡ä»¶å¤¹"""
        import os
        import json
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if step_idx is not None:
            log_filename = f"token_pruning_step_{step_idx:02d}_{timestamp}.json"
        else:
            log_filename = f"token_pruning_summary_{timestamp}.json"
        
        log_path = os.path.join(output_dir, log_filename)
        
        # å‡†å¤‡æ—¥å¿—æ•°æ®
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'step_index': step_idx,
            'pruning_summary': self.get_pruning_summary(),
            'expert_name': self.expert_name,
            'configuration': {
                'baseline_steps': self.baseline_steps,
                'percentile_threshold': self.percentile_threshold,
                'start_layer': self.start_layer,
                'end_layer': self.end_layer
            }
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        return log_path

    def generate_pruning_summary_report(self, output_dir):
        """ç”Ÿæˆå®Œæ•´çš„è£å‰ªè¿‡ç¨‹æ±‡æ€»æŠ¥å‘Š"""
        import os
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"token_pruning_summary_report_{timestamp}.txt"
        report_path = os.path.join(output_dir, report_filename)
        
        # å‡†å¤‡æ±‡æ€»æ•°æ®
        summary = self.get_pruning_summary()
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("WAN2.2 Tokenä¿®å‰ªè¿‡ç¨‹è¯¦ç»†æŠ¥å‘Š (åŸºäºçœŸå®Latentå˜åŒ–)\n")
            f.write("=" * 80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ä¸“å®¶ç±»å‹: {self.expert_name}\n")
            f.write("\n")
            
            # é…ç½®ä¿¡æ¯
            f.write("ğŸ“‹ é…ç½®å‚æ•°:\n")
            f.write("-" * 40 + "\n")
            f.write(f"åŸºå‡†æ­¥æ•°: {self.baseline_steps}\n")
            f.write(f"ç™¾åˆ†ä½é˜ˆå€¼: {self.percentile_threshold}%\n")
            f.write(f"åŠ¨æ€é˜ˆå€¼: {self.dynamic_threshold:.4f}\n" if self.dynamic_threshold else "åŠ¨æ€é˜ˆå€¼: æœªç¡®å®š\n")
            f.write(f"å¼€å§‹å±‚æ•°: {self.start_layer}\n")
            f.write(f"ç»“æŸå±‚æ•°: {self.end_layer}\n")
            f.write("è¯„åˆ†æ–¹å¼: ä»…åŸºäºçœŸå®Latentå˜åŒ–ï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿ\n")
            f.write("\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡:\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»å†»ç»“Tokenæ•°: {summary['total_frozen_tokens']}\n")
            f.write(f"å˜åŒ–åˆ†æ•°ç»Ÿè®¡: min={summary['change_score_stats'].get('min', 0):.4f}, "
                   f"max={summary['change_score_stats'].get('max', 0):.4f}, "
                   f"avg={summary['change_score_stats'].get('sum', 0) / max(summary['change_score_stats'].get('count', 1), 1):.4f}\n")
            f.write(f"ç»Ÿè®¡æ ·æœ¬æ•°: {summary['change_score_stats'].get('count', 0)} ä¸ªçœŸå®tokenå˜åŒ–å€¼\n")
            f.write("\n")
            
            # å†»ç»“Tokenåˆ—è¡¨
            f.write("ğŸ§Š å†»ç»“Tokenåˆ—è¡¨:\n")
            f.write("-" * 40 + "\n")
            frozen_tokens = sorted(summary['frozen_token_list'])
            for i, token_idx in enumerate(frozen_tokens):
                if i % 10 == 0 and i > 0:
                    f.write("\n")
                f.write(f"{token_idx:4d} ")
            f.write(f"\n\nå…± {len(frozen_tokens)} ä¸ªTokenè¢«å†»ç»“\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ - 100%åŸºäºçœŸå®æ•°æ®ï¼Œé›¶æ¨¡æ‹Ÿ\n")
            f.write("=" * 80 + "\n")
        
        print(f"ğŸ“„ è£å‰ªæ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
