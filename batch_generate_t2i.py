"""
æ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡è„šæœ¬
ä»MS-COCO prompts.csvè¯»å–æç¤ºè¯ï¼Œæ‰¹é‡ç”Ÿæˆå•å¸§å›¾ç‰‡ç”¨äºè¯„ä¼°
"""
import os
import sys
import argparse
import pandas as pd
import torch
from pathlib import Path
from tqdm import tqdm
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„ - Wan2.2å­ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(script_dir, 'Wan2.2'))
sys.path.insert(0, script_dir)

from generate import setup_model, parse_args


def batch_generate_t2i(
    prompts_csv_path: str,
    output_dir: str,
    num_samples: int = None,
    seed_start: int = 42,
    num_inference_steps: int = 20,
    guidance_scale: float = 7.5,
    height: int = 480,
    width: int = 832,
    enable_half_frame: bool = False,
    cfg_truncation_step: int = None,
    model_path: str = None,
    device: str = "cuda:0"
):
    """
    æ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡
    
    Args:
        prompts_csv_path: MS-COCO prompts CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        seed_start: èµ·å§‹éšæœºç§å­
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: CFGå¼•å¯¼å¼ºåº¦
        height: å›¾ç‰‡é«˜åº¦
        width: å›¾ç‰‡å®½åº¦
        enable_half_frame: æ˜¯å¦å¯ç”¨å¸§æ•°å‡åŠä¼˜åŒ–
        cfg_truncation_step: CFGæˆªæ–­æ­¥æ•°
        model_path: æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è¯»å–prompts
    print(f"ğŸ“– è¯»å–promptsæ–‡ä»¶: {prompts_csv_path}")
    df = pd.read_csv(prompts_csv_path)
    
    # æ£€æŸ¥CSVæ ¼å¼
    if 'prompt' not in df.columns and 'caption' not in df.columns:
        raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'prompt' æˆ– 'caption' åˆ—")
    
    prompt_column = 'prompt' if 'prompt' in df.columns else 'caption'
    prompts = df[prompt_column].tolist()
    
    # å¦‚æœæœ‰image_idåˆ—ï¼Œä½¿ç”¨å®ƒä½œä¸ºæ–‡ä»¶å
    if 'image_id' in df.columns:
        image_ids = df['image_id'].tolist()
    else:
        image_ids = [f"{i:06d}" for i in range(len(prompts))]
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if num_samples is not None:
        prompts = prompts[:num_samples]
        image_ids = image_ids[:num_samples]
    
    print(f"âœ… å…±åŠ è½½ {len(prompts)} ä¸ªprompts")
    
    # è®¾ç½®æ¨¡å‹ï¼ˆä½¿ç”¨generate.pyä¸­çš„setupå‡½æ•°ï¼‰
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹...")
    
    # æ„é€ å‚æ•°å¯¹è±¡
    class Args:
        def __init__(self):
            self.model_path = model_path or "path/to/model"
            self.prompt = ""  # ä¸´æ—¶å ä½
            self.negative_prompt = ""
            self.height = height
            self.width = width
            self.num_frames = 1  # T2Iåªç”Ÿæˆ1å¸§
            self.num_inference_steps = num_inference_steps
            self.guidance_scale = guidance_scale
            self.seed = seed_start
            self.output_dir = output_dir
            self.enable_half_frame_generation = enable_half_frame
            self.cfg_truncation_step = cfg_truncation_step
            self.enable_debug = False
            self.debug_output_dir = None
            self.device = device
            self.dtype = "bf16"
            self.enable_tiling = False
            self.tile_sample_min_height = 0
            self.tile_sample_min_width = 0
            self.enable_vae_tiling = False
            self.vae_tile_sample_min_height = 0
            self.vae_tile_sample_min_width = 0
    
    args = Args()
    
    # åŠ è½½æ¨¡å‹
    try:
        import wan
        from omegaconf import OmegaConf
        
        # åŠ è½½é…ç½®
        config_path = os.path.join(model_path, "config.yaml")
        if not os.path.exists(config_path):
            # å°è¯•é»˜è®¤é…ç½®è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(script_dir, "Wan2.2/configs/t2v_A14B.yaml")
        
        cfg = OmegaConf.load(config_path)
        
        # æå–deviceç¼–å·
        device_id = int(device.split(':')[1]) if ':' in device else 0
        
        # åˆå§‹åŒ–æ¨¡å‹
        wan_t2v = wan.WanT2V(
            config=cfg,
            checkpoint_dir=model_path,
            device_id=device_id,
            rank=device_id,
            t5_fsdp=False
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”å·²å®‰è£…æ‰€æœ‰ä¾èµ–")
        import traceback
        traceback.print_exc()
        return
    
    # æ‰¹é‡ç”Ÿæˆ
    print(f"\nğŸ¨ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(prompts)} å¼ å›¾ç‰‡...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"âš™ï¸  é…ç½®: {height}x{width}, steps={num_inference_steps}, cfg={guidance_scale}")
    
    success_count = 0
    failed_prompts = []
    
    for idx, (prompt, image_id) in enumerate(tqdm(zip(prompts, image_ids), total=len(prompts))):
        try:
            # è®¾ç½®å½“å‰promptå’Œseed
            args.prompt = prompt
            args.seed = seed_start + idx
            
            # è¾“å‡ºæ–‡ä»¶å
            output_filename = f"{image_id}_seed{args.seed}.png"
            output_path = os.path.join(output_dir, output_filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if os.path.exists(output_path):
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {output_filename}")
                success_count += 1
                continue
            
            # ç”Ÿæˆå›¾ç‰‡
            start_time = time.time()
            
            video, timing_info = wan_t2v.generate(
                prompt=args.prompt,
                negative_prompt=args.negative_prompt,
                height=args.height,
                width=args.width,
                num_frames=args.num_frames,
                num_inference_steps=args.num_inference_steps,
                guidance_scale=args.guidance_scale,
                seed=args.seed,
                enable_half_frame_generation=args.enable_half_frame_generation,
                cfg_truncation_step=args.cfg_truncation_step,
                enable_debug=False
            )
            
            generation_time = time.time() - start_time
            
            # ä¿å­˜å›¾ç‰‡ï¼ˆvideoæ˜¯tensorï¼Œshape: [B, F, C, H, W]ï¼‰
            import torchvision
            frame = video[0, 0]  # å–ç¬¬ä¸€å¸§
            torchvision.utils.save_image(frame, output_path, normalize=True, value_range=(-1, 1))
            
            success_count += 1
            
            if (idx + 1) % 10 == 0:
                print(f"\nâœ… å·²å®Œæˆ {success_count}/{len(prompts)} å¼ å›¾ç‰‡")
                print(f"   æœ€è¿‘ä¸€å¼ è€—æ—¶: {generation_time:.2f}ç§’")
        
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥ [{idx+1}/{len(prompts)}]: {prompt[:50]}...")
            print(f"   é”™è¯¯: {e}")
            failed_prompts.append((idx, prompt, str(e)))
            continue
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {success_count}/{len(prompts)}")
    print(f"âŒ å¤±è´¥: {len(failed_prompts)}/{len(prompts)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_prompts:
        failed_log_path = os.path.join(output_dir, "failed_prompts.txt")
        with open(failed_log_path, 'w', encoding='utf-8') as f:
            for idx, prompt, error in failed_prompts:
                f.write(f"[{idx}] {prompt}\n")
                f.write(f"Error: {error}\n\n")
        print(f"ğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_log_path}")


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡ç”¨äºè¯„ä¼°")
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument("--prompts_csv", type=str, required=True,
                        help="MS-COCO prompts CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="è¾“å‡ºå›¾ç‰‡ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=None,
                        help="ç”Ÿæˆæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, required=True,
                        help="æ¨¡å‹checkpointç›®å½•è·¯å¾„ï¼ˆåŒ…å«æƒé‡å’Œconfig.yamlï¼‰")
    parser.add_argument("--device", type=str, default="cuda:0",
                        help="è®¾å¤‡")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--seed_start", type=int, default=42,
                        help="èµ·å§‹éšæœºç§å­")
    parser.add_argument("--num_inference_steps", type=int, default=20,
                        help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance_scale", type=float, default=7.5,
                        help="CFGå¼•å¯¼å¼ºåº¦")
    parser.add_argument("--height", type=int, default=480,
                        help="å›¾ç‰‡é«˜åº¦")
    parser.add_argument("--width", type=int, default=832,
                        help="å›¾ç‰‡å®½åº¦")
    
    # ä¼˜åŒ–é€‰é¡¹
    parser.add_argument("--enable_half_frame", action="store_true",
                        help="å¯ç”¨å¸§æ•°å‡åŠä¼˜åŒ–")
    parser.add_argument("--cfg_truncation_step", type=int, default=None,
                        help="CFGæˆªæ–­æ­¥æ•°")
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ
    batch_generate_t2i(
        prompts_csv_path=args.prompts_csv,
        output_dir=args.output_dir,
        num_samples=args.num_samples,
        seed_start=args.seed_start,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        height=args.height,
        width=args.width,
        enable_half_frame=args.enable_half_frame,
        cfg_truncation_step=args.cfg_truncation_step,
        model_path=args.model_path,
        device=args.device
    )


if __name__ == "__main__":
    main()

