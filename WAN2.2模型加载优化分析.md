# WAN2.2æ¨¡å‹åŠ è½½æ—¶é—´è¿‡é•¿é—®é¢˜åˆ†æä¸ä¼˜åŒ–

## ğŸ” **åŠ è½½æ—¶é—´è¿‡é•¿çš„åŸå› åˆ†æ**

### **1. å¤šä¸ªå¤§å‹æ¨¡å‹é¡ºåºåŠ è½½**

**27B MOEæ¨¡å‹åŠ è½½è¿‡ç¨‹**ï¼š
```python
# åŠ è½½é¡ºåºå’Œæ—¶é—´ä¼°ç®—
self.text_encoder = T5EncoderModel(...)          # ~4GB, è€—æ—¶30-60ç§’
self.vae = Wan2_1_VAE(...)                      # ~1GB, è€—æ—¶10-20ç§’  
self.low_noise_model = WanModel.from_pretrained(...)   # ~14GB, è€—æ—¶60-120ç§’
self.high_noise_model = WanModel.from_pretrained(...)  # ~14GB, è€—æ—¶60-120ç§’

# æ€»åŠ è½½æ—¶é—´: 160-320ç§’ (2.5-5åˆ†é’Ÿ)
```

### **2. CPUâ†”GPUæ•°æ®ä¼ è¾“ç“¶é¢ˆ**

**è®¾å¤‡è½¬ç§»å¼€é”€**ï¼š
```python
# æ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦CPUâ†’GPUä¼ è¾“
model.to(self.device)  # PCIe 4.0 x16: ~32GB/s
# 14GBæ¨¡å‹ä¼ è¾“æ—¶é—´: 14GB Ã· 32GB/s â‰ˆ 0.4ç§’ Ã— 4ä¸ªæ¨¡å‹ = 1.6ç§’
```

### **3. æƒé‡æ–‡ä»¶I/Oç“¶é¢ˆ**

**ç£ç›˜è¯»å–é€Ÿåº¦**ï¼š
```python
torch.load(checkpoint_path, map_location='cpu')
# SSDè¯»å–é€Ÿåº¦: ~3-7GB/s
# 27Bæ¨¡å‹æƒé‡è¯»å–: ~111GB Ã· 5GB/s â‰ˆ 22ç§’
# HDDè¯»å–é€Ÿåº¦: ~0.1-0.2GB/s  
# 27Bæ¨¡å‹æƒé‡è¯»å–: ~111GB Ã· 0.15GB/s â‰ˆ 740ç§’ (12åˆ†é’Ÿ)
```

### **4. æ¨¡å‹åˆå§‹åŒ–å’Œé…ç½®**

**é¢å¤–å¼€é”€**ï¼š
```python
# FSDPåˆ†ç‰‡åˆå§‹åŒ–
if dit_fsdp:
    model = shard_fn(model)  # 5-15ç§’

# æ•°æ®ç±»å‹è½¬æ¢
if convert_model_dtype:
    model.to(self.param_dtype)  # 2-5ç§’

# åºåˆ—å¹¶è¡Œé…ç½®
if use_sp:
    # æ³¨å†Œåˆ†å¸ƒå¼hook # 1-3ç§’
```

## âš¡ **ä¼˜åŒ–æ–¹æ¡ˆ**

### **æ–¹æ¡ˆ1: å¹¶è¡ŒåŠ è½½æ¨¡å‹**
```python
import concurrent.futures
import threading

def load_models_parallel(self):
    """å¹¶è¡ŒåŠ è½½å¤šä¸ªæ¨¡å‹ç»„ä»¶"""
    
    def load_t5():
        return T5EncoderModel(...)
    
    def load_vae():
        return Wan2_1_VAE(...)
    
    def load_low_noise():
        return WanModel.from_pretrained(..., subfolder='low_noise_model')
    
    def load_high_noise():
        return WanModel.from_pretrained(..., subfolder='high_noise_model')
    
    # å¹¶è¡ŒåŠ è½½
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            't5': executor.submit(load_t5),
            'vae': executor.submit(load_vae),
            'low_noise': executor.submit(load_low_noise),
            'high_noise': executor.submit(load_high_noise)
        }
        
        # ç­‰å¾…æ‰€æœ‰åŠ è½½å®Œæˆ
        results = {name: future.result() for name, future in futures.items()}
    
    # é¢„æœŸåŠ é€Ÿ: 2.5-4x (å¹¶è¡ŒåŠ è½½)
```

### **æ–¹æ¡ˆ2: å»¶è¿ŸåŠ è½½ç­–ç•¥**
```python
def lazy_loading_strategy(self):
    """å»¶è¿ŸåŠ è½½ï¼šåªåœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹"""
    
    # å¯åŠ¨æ—¶åªåŠ è½½å¿…è¦ç»„ä»¶
    self.text_encoder = T5EncoderModel(...)  # å¿…é¡»é¢„åŠ è½½
    self.vae = Wan2_1_VAE(...)              # å¿…é¡»é¢„åŠ è½½
    
    # ä¸“å®¶æ¨¡å‹å»¶è¿ŸåŠ è½½
    self.low_noise_model = None
    self.high_noise_model = None
    self._low_noise_loaded = False
    self._high_noise_loaded = False
    
    def _load_expert_on_demand(self, expert_type):
        if expert_type == 'low_noise' and not self._low_noise_loaded:
            self.low_noise_model = WanModel.from_pretrained(...)
            self._low_noise_loaded = True
        elif expert_type == 'high_noise' and not self._high_noise_loaded:
            self.high_noise_model = WanModel.from_pretrained(...)
            self._high_noise_loaded = True
    
    # é¢„æœŸæ•ˆæœ: å¯åŠ¨æ—¶é—´å‡å°‘50-70%
```

### **æ–¹æ¡ˆ3: æ¨¡å‹ç¼“å­˜å’Œé¢„çƒ­**
```python
def model_caching_strategy(self):
    """æ¨¡å‹ç¼“å­˜ï¼šé¿å…é‡å¤åŠ è½½"""
    
    import pickle
    cache_dir = Path("./model_cache")
    cache_dir.mkdir(exist_ok=True)
    
    def load_with_cache(model_path, cache_key):
        cache_file = cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            print(f"ä»ç¼“å­˜åŠ è½½ {cache_key}...")
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        else:
            print(f"é¦–æ¬¡åŠ è½½ {cache_key}...")
            model = WanModel.from_pretrained(model_path)
            # ç¼“å­˜æ¨¡å‹çŠ¶æ€
            with open(cache_file, 'wb') as f:
                pickle.dump(model.state_dict(), f)
            return model
    
    # é¢„æœŸæ•ˆæœ: ç¬¬äºŒæ¬¡å¯åŠ¨åŠ é€Ÿ80%+
```

### **æ–¹æ¡ˆ4: å­˜å‚¨ä¼˜åŒ–**
```python
def storage_optimization():
    """å­˜å‚¨å’ŒI/Oä¼˜åŒ–"""
    
    # 1. ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½
    torch.load(checkpoint_path, map_location='cpu', mmap=True)
    
    # 2. é¢„åˆ†é…GPUå†…å­˜
    torch.cuda.empty_cache()
    torch.cuda.set_per_process_memory_fraction(0.9)
    
    # 3. ä½¿ç”¨æ›´å¿«çš„æ•°æ®ç±»å‹
    torch_dtype = torch.bfloat16  # æ¯”float32å¿«2x
    
    # 4. æ‰¹é‡ä¼ è¾“åˆ°GPU
    with torch.cuda.stream(torch.cuda.Stream()):
        model.to(device, non_blocking=True)
```

## ğŸš€ **æ¨èçš„å¿«é€Ÿå¯åŠ¨é…ç½®**

### **é…ç½®1: æœ€å°å†…å­˜å ç”¨ï¼ˆæ…¢å¯åŠ¨ï¼‰**
```bash
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --offload_model True \      # æ¨¡å‹å¸è½½ï¼Œå‡å°‘å†…å­˜ä½†å¢åŠ åŠ è½½æ—¶é—´
    --convert_model_dtype \     # æ•°æ®ç±»å‹è½¬æ¢ï¼Œé¢å¤–å¼€é”€
    --t5_cpu                    # T5åœ¨CPUï¼Œå‡å°‘GPUå†…å­˜ä½†å¢åŠ ä¼ è¾“æ—¶é—´
```

### **é…ç½®2: å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰**
```bash
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --convert_model_dtype       # ä¿ç•™æ•°æ®ç±»å‹è½¬æ¢
    # ç§»é™¤offload_modelå’Œt5_cpuï¼Œå‡å°‘è¿è¡Œæ—¶ä¼ è¾“
```

### **é…ç½®3: æœ€å¿«å¯åŠ¨ï¼ˆéœ€è¦æ›´å¤šå†…å­˜ï¼‰**
```bash
python generate.py --task t2v-A14B \
    --ckpt_dir ./model_weights
    # æ— ä»»ä½•ä¼˜åŒ–å‚æ•°ï¼Œæ‰€æœ‰æ¨¡å‹å¸¸é©»GPU
```

### **é…ç½®4: å¤šGPUæœ€ä¼˜ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰**
```bash
torchrun --nproc_per_node=4 generate.py \
    --task t2v-A14B \
    --ckpt_dir ./model_weights \
    --dit_fsdp \               # æ¨¡å‹åˆ†ç‰‡ï¼Œå‡å°‘å•å¡åŠ è½½æ—¶é—´
    --t5_fsdp \                # T5åˆ†ç‰‡
    --ulysses_size 4           # åºåˆ—å¹¶è¡Œï¼ŒåŠ é€Ÿæ¨ç†
```

## ğŸ“Š **æ€§èƒ½å¯¹æ¯”**

| é…ç½® | å¯åŠ¨æ—¶é—´ | å†…å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|----------|
| **æœ€å°å†…å­˜** | 5-8åˆ†é’Ÿ | 40GB | æ…¢ | å†…å­˜å—é™ |
| **å¹³è¡¡æ¨¡å¼** | 3-5åˆ†é’Ÿ | 60GB | ä¸­ç­‰ | ä¸€èˆ¬ä½¿ç”¨ |
| **æœ€å¿«å¯åŠ¨** | 2-3åˆ†é’Ÿ | 80GB+ | å¿« | å†…å­˜å……è¶³ |
| **å¤šGPU** | 1-2åˆ†é’Ÿ | 20GB/å¡ | æœ€å¿« | ç”Ÿäº§ç¯å¢ƒ |

## ğŸ”§ **ç«‹å³å¯ç”¨çš„ä¼˜åŒ–æŠ€å·§**

### **1. é¢„çƒ­GPU**
```bash
# åœ¨åŠ è½½æ¨¡å‹å‰é¢„çƒ­GPU
python -c "import torch; torch.cuda.init(); torch.cuda.empty_cache()"
```

### **2. è®¾ç½®ç¯å¢ƒå˜é‡**
```bash
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=0
export TOKENIZERS_PARALLELISM=false
```

### **3. ä½¿ç”¨RAM Diskï¼ˆLinux/Macï¼‰**
```bash
# å°†æƒé‡æ–‡ä»¶æ”¾åœ¨å†…å­˜ä¸­
sudo mount -t tmpfs -o size=120G tmpfs /tmp/model_weights
cp -r ./model_weights/* /tmp/model_weights/
# ä½¿ç”¨ /tmp/model_weights ä½œä¸º ckpt_dir
```

### **4. SSDä¼˜åŒ–**
- ç¡®ä¿æ¨¡å‹æƒé‡å­˜å‚¨åœ¨NVMe SSDä¸Š
- é¿å…ä½¿ç”¨æœºæ¢°ç¡¬ç›˜å­˜å‚¨æƒé‡æ–‡ä»¶
- è€ƒè™‘ä½¿ç”¨RAID 0æé«˜I/Oæ€§èƒ½

---

**æ€»ç»“**: åŠ è½½æ—¶é—´ä¸»è¦å—é™äºç£ç›˜I/Oã€CPU-GPUä¼ è¾“å’Œæ¨¡å‹åˆå§‹åŒ–ã€‚é€šè¿‡å¹¶è¡ŒåŠ è½½ã€å»¶è¿ŸåŠ è½½å’Œå­˜å‚¨ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æ”¹å–„å¯åŠ¨æ€§èƒ½ã€‚
