#!/usr/bin/env python3
"""
WAN2.2 æ¨ç†è°ƒè¯•å’Œé”™è¯¯æ¢å¤è„šæœ¬
ç”¨äºè¯Šæ–­å’Œè§£å†³å¸¸è§çš„æ¨ç†é—®é¢˜
"""

import argparse
import os
import subprocess
import sys
import torch
from pathlib import Path

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ” GPUçŠ¶æ€æ£€æŸ¥")
    print("-" * 40)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_total = props.total_memory / 1024**3
        memory_free = torch.cuda.get_device_properties(i).total_memory / 1024**3
        try:
            torch.cuda.set_device(i)
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3
            memory_free = memory_total - memory_allocated
        except:
            memory_allocated = 0
            memory_cached = 0
            memory_free = memory_total
            
        print(f"  GPU {i}: {props.name}")
        print(f"    æ€»å†…å­˜: {memory_total:.1f}GB")
        print(f"    å·²ç”¨å†…å­˜: {memory_allocated:.1f}GB")
        print(f"    ç¼“å­˜å†…å­˜: {memory_cached:.1f}GB") 
        print(f"    å¯ç”¨å†…å­˜: {memory_free:.1f}GB")
    
    return True

def test_simple_inference(args):
    """æµ‹è¯•ç®€å•æ¨ç†"""
    print("\nğŸ§ª ç®€å•æ¨ç†æµ‹è¯•")
    print("-" * 40)
    
    # æœ€ä¿å®ˆçš„é…ç½®
    cmd = [
        sys.executable, "generate.py",
        "--task", args.task,
        "--size", "1280*720", 
        "--ckpt_dir", args.ckpt_dir,
        "--frame_num", "1",
        "--offload_model", "True",
        "--convert_model_dtype",
        "--t5_cpu",
        "--prompt", "test"
    ]
    
    print(f"ğŸ“ æµ‹è¯•å‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, cwd="Wan2.2", capture_output=True, text=True, timeout=600)
        if result.returncode == 0:
            print("âœ… ç®€å•æ¨ç†æµ‹è¯•æˆåŠŸ")
            return True
        else:
            print("âŒ ç®€å•æ¨ç†æµ‹è¯•å¤±è´¥")
            print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â° æµ‹è¯•è¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def suggest_optimal_config(gpu_memory_gb):
    """æ ¹æ®GPUå†…å­˜å»ºè®®æœ€ä¼˜é…ç½®"""
    print(f"\nğŸ’¡ åŸºäº{gpu_memory_gb:.1f}GB GPUå†…å­˜çš„æ¨èé…ç½®")
    print("-" * 50)
    
    if gpu_memory_gb >= 80:
        print("ğŸš€ é«˜ç«¯é…ç½® (A100 80GB+):")
        print("  python generate.py --task t2v-A14B --fast_loading")
        print("  # æˆ–å¤šGPU: torchrun --nproc_per_node=4 --dit_fsdp --t5_fsdp")
        
    elif gpu_memory_gb >= 40:
        print("âš¡ ä¸­ç«¯é…ç½® (A100 40GB):")
        print("  python generate.py --task t2v-A14B --offload_model True --convert_model_dtype")
        
    elif gpu_memory_gb >= 24:
        print("ğŸ’¾ è½»é‡é…ç½® (RTX 4090):")
        print("  python generate.py --task ti2v-5B --offload_model True --t5_cpu")
        
    else:
        print("âš ï¸ å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨äº‘å¹³å°")

def main():
    parser = argparse.ArgumentParser(description="WAN2.2æ¨ç†è°ƒè¯•å·¥å…·")
    parser.add_argument("--task", default="t2v-A14B", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--ckpt_dir", default="../WAN2.2-27B/T2V_A14B_weights", help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--test_inference", action="store_true", help="è¿è¡Œæ¨ç†æµ‹è¯•")
    
    args = parser.parse_args()
    
    print("ğŸ”§ WAN2.2 æ¨ç†è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥GPUçŠ¶æ€
    if not check_gpu_status():
        return
    
    # è·å–GPUå†…å­˜ä¿¡æ¯
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    suggest_optimal_config(gpu_memory)
    
    # å¯é€‰çš„æ¨ç†æµ‹è¯•
    if args.test_inference:
        test_simple_inference(args)
    
    print("\nğŸ“‹ å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ:")
    print("1. SIGSEGV (-11): ä½¿ç”¨ --offload_model True --convert_model_dtype")
    print("2. OOM: é™ä½å¹¶è¡Œåº¦æˆ–ä½¿ç”¨ --t5_cpu")
    print("3. åŠ è½½æ…¢: ä½¿ç”¨ --fast_loading æˆ–å¤šGPUåˆ†ç‰‡")
    print("4. ç½‘ç»œé”™è¯¯: æ£€æŸ¥é˜²ç«å¢™å’ŒNCCLé…ç½®")

if __name__ == "__main__":
    main()
