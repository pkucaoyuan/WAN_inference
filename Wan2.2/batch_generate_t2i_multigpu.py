"""
å¤šGPUæ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡è„šæœ¬
æ¯å¼ å¡å¤„ç†ä¸åŒçš„æ ·æœ¬ï¼Œå®ç°å¹¶è¡ŒåŠ é€Ÿ
"""
import os
import sys
import argparse
import pandas as pd
import torch
import torch.multiprocessing as mp
from pathlib import Path
from tqdm import tqdm
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


def worker_generate(
    rank: int,
    world_size: int,
    prompts_csv_path: str,
    output_dir: str,
    num_samples: int,
    seed_start: int,
    num_inference_steps: int,
    guidance_scale: float,
    height: int,
    width: int,
    enable_half_frame: bool,
    cfg_truncation_step: int,
    model_path: str,
    negative_prompt: str,
    dtype: str
):
    """
    å•ä¸ªGPU workerçš„ç”Ÿæˆå‡½æ•°
    
    Args:
        rank: GPUç¼–å·
        world_size: æ€»GPUæ•°é‡
        å…¶ä»–å‚æ•°åŒbatch_generate_t2i
    """
    device = f"cuda:{rank}"
    
    print(f"[GPU {rank}] ğŸš€ å¯åŠ¨workerï¼Œè®¾å¤‡: {device}")
    
    # è¯»å–prompts
    df = pd.read_csv(prompts_csv_path)
    
    # æ£€æŸ¥CSVæ ¼å¼
    if 'prompt' not in df.columns and 'caption' not in df.columns:
        raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'prompt' æˆ– 'caption' åˆ—")
    
    prompt_column = 'prompt' if 'prompt' in df.columns else 'caption'
    prompts = df[prompt_column].tolist()
    
    # å¦‚æœæœ‰image_idåˆ—ï¼Œä½¿ç”¨å®ƒä½œä¸ºæ–‡ä»¶å
    if 'image_id' in df.columns:
        image_ids = df['image_id'].tolist()
    else:
        image_ids = [f"{i:06d}" for i in range(len(prompts))]
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if num_samples is not None:
        prompts = prompts[:num_samples]
        image_ids = image_ids[:num_samples]
    
    # ä»»åŠ¡åˆ†é…ï¼šæ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ ·æœ¬
    # ä½¿ç”¨ç®€å•çš„è½®è¯¢åˆ†é…ç­–ç•¥
    my_indices = list(range(rank, len(prompts), world_size))
    my_prompts = [prompts[i] for i in my_indices]
    my_image_ids = [image_ids[i] for i in my_indices]
    
    print(f"[GPU {rank}] ğŸ“‹ åˆ†é…åˆ° {len(my_prompts)} ä¸ªä»»åŠ¡ (æ€»å…± {len(prompts)} ä¸ª)")
    print(f"[GPU {rank}] ğŸ“ ä»»åŠ¡ç´¢å¼•èŒƒå›´: {my_indices[0]} åˆ° {my_indices[-1]} (æ­¥é•¿ {world_size})")
    
    # åŠ è½½æ¨¡å‹
    print(f"[GPU {rank}] ğŸ”§ åŠ è½½æ¨¡å‹...")
    
    try:
        import wan
        from omegaconf import OmegaConf
        
        # åŠ è½½é…ç½®
        config_path = os.path.join(model_path, "config.yaml")
        if not os.path.exists(config_path):
            # å°è¯•é»˜è®¤é…ç½®è·¯å¾„
            config_path = "Wan2.2/configs/t2v_A14B.yaml"
        
        cfg = OmegaConf.load(config_path)
        
        # åˆå§‹åŒ–æ¨¡å‹
        wan_t2v = wan.WanT2V(
            config=cfg,
            checkpoint_dir=model_path,
            device_id=rank,
            rank=rank,
            t5_fsdp=False
        )
        
        print(f"[GPU {rank}] âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"[GPU {rank}] âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ‰¹é‡ç”Ÿæˆ
    print(f"[GPU {rank}] ğŸ¨ å¼€å§‹ç”Ÿæˆ...")
    
    success_count = 0
    failed_prompts = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯ä¸ªGPUç‹¬ç«‹çš„è¿›åº¦æ¡ï¼‰
    pbar = tqdm(
        zip(my_prompts, my_image_ids, my_indices),
        total=len(my_prompts),
        desc=f"GPU {rank}",
        position=rank,
        leave=True
    )
    
    for prompt, image_id, original_idx in pbar:
        try:
            # è®¡ç®—seedï¼ˆä¿æŒä¸å•GPUç‰ˆæœ¬ä¸€è‡´ï¼‰
            seed = seed_start + original_idx
            
            # è¾“å‡ºæ–‡ä»¶å
            output_filename = f"{image_id}_seed{seed}.png"
            output_path = os.path.join(output_dir, output_filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if os.path.exists(output_path):
                success_count += 1
                pbar.set_postfix({"success": success_count, "failed": len(failed_prompts)})
                continue
            
            # ç”Ÿæˆå›¾ç‰‡
            video, timing_info = wan_t2v.generate(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=height,
                width=width,
                num_frames=1,  # T2Iåªç”Ÿæˆ1å¸§
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed,
                enable_half_frame_generation=enable_half_frame,
                cfg_truncation_step=cfg_truncation_step,
                enable_debug=False
            )
            
            # ä¿å­˜å›¾ç‰‡
            import torchvision
            frame = video[0, 0]  # å–ç¬¬ä¸€å¸§
            torchvision.utils.save_image(frame, output_path, normalize=True, value_range=(-1, 1))
            
            success_count += 1
            pbar.set_postfix({"success": success_count, "failed": len(failed_prompts)})
        
        except Exception as e:
            print(f"\n[GPU {rank}] âŒ ç”Ÿæˆå¤±è´¥ [ç´¢å¼• {original_idx}]: {prompt[:50]}...")
            print(f"[GPU {rank}]    é”™è¯¯: {e}")
            failed_prompts.append((original_idx, prompt, str(e)))
            pbar.set_postfix({"success": success_count, "failed": len(failed_prompts)})
            continue
    
    pbar.close()
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\n[GPU {rank}] {'='*60}")
    print(f"[GPU {rank}] ğŸ‰ ç”Ÿæˆå®Œæˆ!")
    print(f"[GPU {rank}] âœ… æˆåŠŸ: {success_count}/{len(my_prompts)}")
    print(f"[GPU {rank}] âŒ å¤±è´¥: {len(failed_prompts)}/{len(my_prompts)}")
    
    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_prompts:
        failed_log_path = os.path.join(output_dir, f"failed_prompts_gpu{rank}.txt")
        with open(failed_log_path, 'w', encoding='utf-8') as f:
            for idx, prompt, error in failed_prompts:
                f.write(f"[{idx}] {prompt}\n")
                f.write(f"Error: {error}\n\n")
        print(f"[GPU {rank}] ğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_log_path}")


def batch_generate_t2i_multigpu(
    prompts_csv_path: str,
    output_dir: str,
    num_samples: int = None,
    seed_start: int = 42,
    num_inference_steps: int = 20,
    guidance_scale: float = 7.5,
    height: int = 480,
    width: int = 832,
    enable_half_frame: bool = False,
    cfg_truncation_step: int = None,
    model_path: str = None,
    negative_prompt: str = "",
    dtype: str = "bf16",
    gpu_ids: list = None
):
    """
    å¤šGPUæ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡
    
    Args:
        prompts_csv_path: MS-COCO prompts CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        seed_start: èµ·å§‹éšæœºç§å­
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: CFGå¼•å¯¼å¼ºåº¦
        height: å›¾ç‰‡é«˜åº¦
        width: å›¾ç‰‡å®½åº¦
        enable_half_frame: æ˜¯å¦å¯ç”¨å¸§æ•°å‡åŠä¼˜åŒ–
        cfg_truncation_step: CFGæˆªæ–­æ­¥æ•°
        model_path: æ¨¡å‹checkpointç›®å½•è·¯å¾„ï¼ˆåŒ…å«æƒé‡å’Œconfig.yamlï¼‰
        negative_prompt: è´Ÿé¢æç¤ºè¯
        dtype: æ•°æ®ç±»å‹
        gpu_ids: GPU IDåˆ—è¡¨ï¼ˆä¾‹å¦‚[0,1,2,3]ï¼‰
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¡®å®šä½¿ç”¨çš„GPU
    if gpu_ids is None:
        # è‡ªåŠ¨æ£€æµ‹å¯ç”¨GPU
        num_gpus = torch.cuda.device_count()
        gpu_ids = list(range(num_gpus))
    else:
        num_gpus = len(gpu_ids)
    
    if num_gpus == 0:
        print("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
        return
    
    print(f"{'='*60}")
    print(f"ğŸš€ å¤šGPUæ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡")
    print(f"{'='*60}")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   Promptsæ–‡ä»¶: {prompts_csv_path}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   æ ·æœ¬æ•°é‡: {num_samples if num_samples else 'å…¨éƒ¨'}")
    print(f"   ä½¿ç”¨GPU: {gpu_ids} (å…± {num_gpus} å¼ )")
    print(f"   æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"   å¼•å¯¼å¼ºåº¦: {guidance_scale}")
    print(f"   å›¾ç‰‡å°ºå¯¸: {height}x{width}")
    print(f"   å¸§æ•°å‡åŠ: {enable_half_frame}")
    print(f"   CFGæˆªæ–­: {cfg_truncation_step if cfg_truncation_step else 'ä¸ä½¿ç”¨'}")
    print(f"{'='*60}\n")
    
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    
    # å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    start_time = time.time()
    
    processes = []
    for i, gpu_id in enumerate(gpu_ids):
        p = mp.Process(
            target=worker_generate,
            args=(
                gpu_id,  # rankä½¿ç”¨å®é™…çš„GPU ID
                num_gpus,
                prompts_csv_path,
                output_dir,
                num_samples,
                seed_start,
                num_inference_steps,
                guidance_scale,
                height,
                width,
                enable_half_frame,
                cfg_truncation_step,
                model_path,
                negative_prompt,
                dtype
            )
        )
        p.start()
        processes.append(p)
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()
    
    total_time = time.time() - start_time
    
    # æ±‡æ€»ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"âœ… æ‰€æœ‰GPUä»»åŠ¡å®Œæˆ!")
    print(f"{'='*60}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ç»Ÿè®¡ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
    image_files = [f for f in os.listdir(output_dir) if f.endswith('.png')]
    print(f"ğŸ–¼ï¸  ç”Ÿæˆå›¾ç‰‡æ•°é‡: {len(image_files)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥è®°å½•
    failed_logs = [f for f in os.listdir(output_dir) if f.startswith('failed_prompts_gpu')]
    if failed_logs:
        print(f"âš ï¸  å‘ç°å¤±è´¥è®°å½•æ–‡ä»¶: {len(failed_logs)} ä¸ª")
        for log_file in failed_logs:
            print(f"   - {log_file}")


def main():
    parser = argparse.ArgumentParser(description="å¤šGPUæ‰¹é‡ç”ŸæˆT2Iå›¾ç‰‡ç”¨äºè¯„ä¼°")
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument("--prompts_csv", type=str, required=True,
                        help="MS-COCO prompts CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="è¾“å‡ºå›¾ç‰‡ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=None,
                        help="ç”Ÿæˆæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, required=True,
                        help="æ¨¡å‹checkpointç›®å½•è·¯å¾„ï¼ˆåŒ…å«æƒé‡å’Œconfig.yamlï¼‰")
    parser.add_argument("--gpu_ids", type=int, nargs='+', default=None,
                        help="ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼Œä¾‹å¦‚: --gpu_ids 0 1 2 3 (é»˜è®¤ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU)")
    parser.add_argument("--dtype", type=str, default="bf16", choices=["bf16", "fp16"],
                        help="æ•°æ®ç±»å‹")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--seed_start", type=int, default=42,
                        help="èµ·å§‹éšæœºç§å­")
    parser.add_argument("--num_inference_steps", type=int, default=20,
                        help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance_scale", type=float, default=7.5,
                        help="CFGå¼•å¯¼å¼ºåº¦")
    parser.add_argument("--height", type=int, default=480,
                        help="å›¾ç‰‡é«˜åº¦")
    parser.add_argument("--width", type=int, default=832,
                        help="å›¾ç‰‡å®½åº¦")
    parser.add_argument("--negative_prompt", type=str, default="",
                        help="è´Ÿé¢æç¤ºè¯")
    
    # ä¼˜åŒ–é€‰é¡¹
    parser.add_argument("--enable_half_frame", action="store_true",
                        help="å¯ç”¨å¸§æ•°å‡åŠä¼˜åŒ–")
    parser.add_argument("--cfg_truncation_step", type=int, default=None,
                        help="CFGæˆªæ–­æ­¥æ•°")
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ
    batch_generate_t2i_multigpu(
        prompts_csv_path=args.prompts_csv,
        output_dir=args.output_dir,
        num_samples=args.num_samples,
        seed_start=args.seed_start,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        height=args.height,
        width=args.width,
        enable_half_frame=args.enable_half_frame,
        cfg_truncation_step=args.cfg_truncation_step,
        model_path=args.model_path,
        negative_prompt=args.negative_prompt,
        dtype=args.dtype,
        gpu_ids=args.gpu_ids
    )


if __name__ == "__main__":
    main()

